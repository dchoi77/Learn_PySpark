{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyspark.mllib\n",
    "\n",
    "pyspark.mllib is a ML package that works on RDDs.\n",
    "\n",
    "\n",
    "## Basic ML procedure using pyspark.mllib\n",
    "\n",
    "```python\n",
    "# Here data is an RDD object whose rows are LabeledPoint objects.\n",
    "# We consider a binary classification problem.\n",
    "\n",
    "# Split data\n",
    "data_train, data_test = data.randomSplit([0.8,0.2])\n",
    "\n",
    "# Train a model\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "model = LogisticRegressionWithLBFGS.train(data_train, iterations=10)\n",
    "\n",
    "# Prediction\n",
    "preds = model.predict(data_test.map(lambda row: row.features))\n",
    "results = data_test.map(lambda row: row.label).zip(preds).map(lambda row: (row[0], row[1]*1.0))\n",
    "\n",
    "# Evaluation\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics \n",
    "scores = BinaryClassificationMetrics(results)\n",
    "print(scores.areaUnderPR)     # precision-recall curve\n",
    "print(scores.areaUnderROC)\n",
    "```\n",
    "\n",
    "## pyspark.mllib.stat.Statistics\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.stat import Statistics\n",
    "```\n",
    "\n",
    "### colStats()\n",
    "\n",
    "colStats(rdd) computes column-wise summary statistics for the input RDD.\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "rdd = sc.parallelize([Vectors.dense([2, 0, 1, -2]),\n",
    "                      Vectors.dense([4, 5, 0,  3]),\n",
    "                      Vectors.dense([6, 7, -3,  8])])\n",
    "cStats = Statistics.colStats(rdd)\n",
    "\n",
    "# The following are all numpy vectors of length 4.\n",
    "cStats.mean()\n",
    "cStats.variance()\n",
    "cStats.count()\n",
    "cStats.numNonzeros()\n",
    "cStats.max()\n",
    "cStats.min()\n",
    "cStats.normL1()\n",
    "cStats.normL2()\n",
    "```\n",
    "\n",
    "If the input is a DataFrame:\n",
    "\n",
    "```python\n",
    "df.show()\n",
    "+---+----+----+\n",
    "| id|   x|   y|\n",
    "+---+----+----+\n",
    "|  1|4.22|5.08|\n",
    "|  2| 5.0|0.58|\n",
    "| ...         |\n",
    "+---+----+----+\n",
    "\n",
    "rdd = df.select('x','y').rdd.map(lambda row: [x for x in row])\n",
    "cStats = Statistics.colStats(rdd)\n",
    "```\n",
    "\n",
    "### corr()\n",
    "\n",
    "```python\n",
    "# Here rdd is the one used in colStats.\n",
    "corrs = Statistics.corr(rdd)        # 4-by-4 np array\n",
    "```\n",
    "\n",
    "### chiSqTest()\n",
    "\n",
    "chiSqTest(observed, expected=None)\n",
    "\n",
    "`observed` cannot contain negative values.\n",
    "\n",
    "If `observed` is a vector containing the observed categorical counts/relative frequencies, conduct Pearson's chi-squared goodness of fit test of the observed data against the expected distribution, or againt the uniform distribution (by default), with each category having an expected frequency of `1 / len(observed)`.\n",
    "\n",
    "If `observed` is matrix, conduct Pearson's independence test on the input contingency matrix, which cannot contain negative entries or columns or rows that sum up to 0.\n",
    "\n",
    "`expected` is a vector containing the expected categorical counts/relative frequencies. `expected` is rescaled if the expected  sum differs from the `observed` sum.\n",
    "\n",
    "If `observed` is an RDD of LabeledPoint, conduct Pearson's independence test for every feature against the label across the input RDD. For each feature, the (feature, label) pairs are converted into a contingency matrix for which the chi-squared statistic is computed. All label and feature values must be categorical.\n",
    "    \n",
    "```python\n",
    "from pyspark.mllib.linalg import Vectors, Matrices\n",
    "\n",
    "observed = Vectors.dense([4, 6, 5])\n",
    "chi = Statistics.chiSqTest(observed)\n",
    "# Try: chi.statistic, chi.pValue, chi.degreesOfFreedom, chi.method, chiu.nullHypothesis\n",
    "\n",
    "observed = Vectors.dense([21, 38, 43, 80])\n",
    "expected = Vectors.dense([3, 5, 7, 20])\n",
    "chi = Statistics.chiSqTest(observed, expected)\n",
    "    \n",
    "data = [LabeledPoint(0.0, Vectors.dense([0.5, 10.0])),\n",
    "        LabeledPoint(0.0, Vectors.dense([1.5, 20.0])),\n",
    "        LabeledPoint(1.0, Vectors.dense([1.5, 30.0])),\n",
    "        LabeledPoint(0.0, Vectors.dense([3.5, 30.0])),\n",
    "        LabeledPoint(0.0, Vectors.dense([3.5, 40.0])),\n",
    "        LabeledPoint(1.0, Vectors.dense([3.5, 40.0])),]\n",
    "rdd = sc.parallelize(data, 4)\n",
    "chi = Statistics.chiSqTest(rdd)   # a list of length 2, since there are two labels.\n",
    "[chi[i].pValue for i in range(len(chi))]\n",
    "[0.6872892787909721, 0.6822703303362126]\n",
    "```\n",
    "\n",
    "\n",
    "##  pyspark.mllib.linalg\n",
    "\n",
    "### Vectors, Matrices\n",
    "\n",
    "```python\n",
    "v = Vectors.dense([1,2,3])\n",
    "v.norm(2)\n",
    "v.dot(Vectors.dense([3,0,-2]))\n",
    "\n",
    "M = Matrices.dense(2, 3, range(6))\n",
    "M.toArray()\n",
    "array([[0., 2., 4.],\n",
    "       [1., 3., 5.]])\n",
    "```\n",
    "\n",
    "## pyspark.mllib.feature\n",
    "\n",
    "### HashingTF()\n",
    "\n",
    "HashingTF(numFeatures=1048576) maps a sequence of terms to their term frequencies using the hashing trick.\n",
    "\n",
    "```python\n",
    "htf = HashingTF()\n",
    "doc = ['a',]*100+['b',]*10+['c',]\n",
    "htf.transform(doc)\n",
    "SparseVector(1048576, {238153: 100.0, 469732: 10.0, 702740: 1.0})\n",
    "```\n",
    "\n",
    "### ChiSqSelector()\n",
    "\n",
    "We can select the most predictable features.\n",
    "\n",
    "```python\n",
    "# rdd_train and rdd_test are RDD objects consisting of LabeledPoint objects.\n",
    "model = ChiSqSelector(5).fit(rdd_train)            # top five features\n",
    "labels = rdd_train.map(lambda row: row.label)\n",
    "features_selected = model.transform(rdd_train).map(lambda row: row.features)\n",
    "rdd_train_selected = labels.zip(features_selected).map(lambda row: LabelPoint(row[0], row[1]))\n",
    "\n",
    "labels = rdd_test.map(lambda row: row.label)\n",
    "features_selected = model.transform(rdd_test).map(lambda row: row.features)\n",
    "rdd_test_selected = labels.zip(features_selected).map(lambda row: LabelPoint(row[0], row[1]))\n",
    "```\n",
    "\n",
    "## pyspark.mllib.regression\n",
    "\n",
    "### LabeledPoint()\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "lp = LabeledPoint(0.0, Vectors.dense([1.5, 20.0]))\n",
    "lp.label     # 0.0\n",
    "lp.features  # DenseVector([1.5, 20.0])\n",
    "```\n",
    "\n",
    "## pyspark.mllib.tree\n",
    "\n",
    "### RandomForest\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "model = RandomForest.trainClassifier(data=rdd_train, numClasses=2, categoricalFeatureInfo={}, numTrees=10, featureSubsetStrategy='auto', impurity='entropy', maxDepth=4, maxBins=50, seed=4042)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark.ml\n",
    "\n",
    "spark.ml is a ML package that works on DataFrames.\n",
    "\n",
    "\n",
    "## Basic ML procedure using spark.ml\n",
    "\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import IntegerType  \n",
    "import pyspark.ml.feature as ft\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Assume data is a DataFrame with columns label, gp, weight, and height,\n",
    "# where gp consists of '1', '2', and '3' and class_label consists of 0 or 1.\n",
    "# If the column label is not 'label', then we need to specify labelCol='label_name' in some functions used below.\n",
    "\n",
    "data = data.withColumn('gp', df['gp'].cast(IntegerType()))     \n",
    "# The column input to OneHotEncoder should be numeric.\n",
    "\n",
    "make_ohe = ft.OneHotEncoder(inputCol='gp', outputCol='gp_ohe')  \n",
    "# It will create a column named gp_ohe during the pipeline\n",
    "\n",
    "make_features = ft.VectorAssembler(inputCols=['weight','height', make_ohe.getOutputCol()], outputCol='features')\n",
    "# change each row consisting of w, h, and c to the sparse vector [w,h,c]\n",
    "# Note make_ohe.getOutputCol() is 'gp_ohe'.\n",
    "# It will create a column named features during the pipeline\n",
    "\n",
    "select_features = ft.ChiSqSelector(\n",
    "    numTopFeatures=2, \n",
    "    featuresCol=make_features.getOutputCol(),\n",
    "    outputCol='selected_features')\n",
    "\n",
    "lr = LogisticRegression(labelCol=select_features.getOutputCol())\n",
    "\n",
    "pipeline = Pipeline(stages=[make_ohe, make_features, select_features, lr])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "     .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "     .addGrid(lr.maxIter, [5, 10, 20]) \\\n",
    "     .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol='probability') \n",
    "# labelCol is 'label' by default.\n",
    "# rawPredictionCol is either 'rawPrediction' and 'probability'.\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
    "# or use\n",
    "# tvs = TrainValidationSplit(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=evaluator)\n",
    "\n",
    "data_train, data_test = data.randomSplit([0.75, 0.25], seed=4042)\n",
    "\n",
    "model = cv.fit(data_train)\n",
    "model_test = model.transform(data_test)           \n",
    "# model_test is a DataFrame whose columns are the columns of df \n",
    "# plus ['features', 'rawPrediction', 'probability', 'prediction']\n",
    "\n",
    "evaluator.evaluate(model_test, {evaluator.metricName: ['areaUnderROC']})\n",
    "evaluator.evaluate(model_test, {evaluator.metricName: ['areaUnderPR']})\n",
    "# We will get the same result when evaluator is defiend with rawPredictionCol='rawPrediction'.\n",
    "\n",
    "# save the pipeline\n",
    "pipeline.write().overwrite().save('path/to/file')\n",
    "pl = Pipeline.load('path/fo/file')\n",
    "model_test = pl.fit(data_train).transform(data_test)\n",
    "\n",
    "# save the fitted pipeline\n",
    "from pyspark.ml import PipelineModel\n",
    "model.write().overwrite().save('path/to/file')\n",
    "pl_fitted = PipelineModel.load('path/to/file')\n",
    "model_test = pl_fitted.transform(data_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spark.ml.feature\n",
    "\n",
    "```python\n",
    "import pyspark.ml.feature as ft\n",
    "```\n",
    "\n",
    "### VectorAssembler()\n",
    "\n",
    " A feature transformer that merges multiple columns into a vector column.\n",
    " \n",
    "```python\n",
    "df = spark.createDataFrame([(1, 0, 3)], [\"a\", \"b\", \"c\"])\n",
    "vecAssembler = ft.VectorAssembler(inputCols=df.columns, outputCol=\"features\")\n",
    "vecAssembler.transform(df).show()     # no fit() function\n",
    "+---+---+---+-------------+\n",
    "|  a|  b|  c|     features|\n",
    "+---+---+---+-------------+\n",
    "|  1|  0|  3|[1.0,0.0,3.0]|\n",
    "+---+---+---+-------------+\n",
    "```\n",
    "\n",
    "### OneHotEncoder()\n",
    "\n",
    "* The last category is not included by default (configurable via dropLast=False),\n",
    "\n",
    "* Column input to OneHotEncoder() must be of type numeric.\n",
    "\n",
    "* When encoding multi-column by using `inputCols` and `outputCols` params, input/output cols come in pairs, specified by the order in the arrays, and each pair is treated independently.\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame([(1,), (5,), (0,), (1,), (3,),], [\"input\"])  \n",
    "# df.input is a numeric column having three categories.\n",
    "\n",
    "ohe = ft.OneHotEncoder()\n",
    "ohe.setInputCols([\"input\"])\n",
    "ohe.setOutputCols([\"output\"])       # or, ohe = ft.OneHoTEncoder(inputCol=\"input\", outputCol=\"output\")\n",
    "\n",
    "ohe.getInputCols()   # ['input']\n",
    "ohe.getOutputCols()  # ['output']\n",
    "\n",
    "df2 = ohe.fit(df).transform(df)\n",
    "df2.show()                 \n",
    "+-----+-------------+\n",
    "|input|       output|\n",
    "+-----+-------------+\n",
    "|    1|(5,[1],[1.0])|    # SparseVector corresponding to (0,1,0,0)\n",
    "|    5|    (5,[],[])|    # SparseVector corresponding to (0,0,0,0)\n",
    "|    0|(5,[0],[1.0])|    # SparseVector corresponding to (1,0,0,0)\n",
    "|    1|(5,[1],[1.0])|\n",
    "|    3|(5,[3],[1.0])|    # SparseVector corresponding to (0,0,0,1)\n",
    "+-----+-------------+\n",
    "```\n",
    "\n",
    "### QuantileDiscretizer()\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame([(float(x),) for x in np.random.rand(100).round(2)], ['x'])\n",
    "discretizer = ft.QuantileDiscretizer(numBuckets=5, inputCol='x', outputCol='x_disc')\n",
    "discretizer.fit(df).transform(df).show(n=10)\n",
    "+----+------+\n",
    "|   x|x_disc|\n",
    "+----+------+\n",
    "|0.54|   2.0|\n",
    "|0.48|   2.0|\n",
    "|0.46|   2.0|\n",
    "|0.02|   0.0|\n",
    "|0.42|   2.0|\n",
    "|0.15|   0.0|\n",
    "|0.35|   1.0|\n",
    "|0.49|   2.0|\n",
    "|0.21|   1.0|\n",
    "|0.76|   4.0|\n",
    "+----+------+\n",
    "\n",
    "# x_disc consists of 0.0, 1.0, 2.0, 3.0, and 4.0.\n",
    "```\n",
    "\n",
    "### StandardScaler()\n",
    "\n",
    "```python\n",
    "df.show()\n",
    "+----+\n",
    "|   x|\n",
    "+----+\n",
    "|4.59|\n",
    "|9.09|\n",
    "|5.97|\n",
    "|2.99|\n",
    "+----+\n",
    "\n",
    "# Use Vectors\n",
    "df = ft.VectorAssembler(inputCols=['x'], outputCol='x_vec').transform(df)\n",
    "df.show()\n",
    "+----+------+\n",
    "|   x| x_vec|\n",
    "+----+------+\n",
    "|4.59|[4.59]|\n",
    "|9.09|[9.09]|\n",
    "|5.97|[5.97]|\n",
    "|2.99|[2.99]|\n",
    "+----+------+\n",
    "\n",
    "scaler = ft.StandardScaler(inputCol='x_vec', outputCol='normalized', withMean=True, withStd=True)\n",
    "scaler.fit(df).transform(df).show()\n",
    "+----+------+--------------------+\n",
    "|   x| x_vec|          normalized|\n",
    "+----+------+--------------------+\n",
    "|4.59|[4.59]|[-0.413019679166438]|\n",
    "|9.09|[9.09]|[1.3239789715335344]|\n",
    "|5.97|[5.97]|[0.11965990704822...|\n",
    "|2.99|[2.99]|[-1.0306191994153...|\n",
    "+----+------+--------------------+\n",
    "```\n",
    "\n",
    "\n",
    "### NLP features\n",
    "\n",
    "* RegexTokenizer()\n",
    "* StopWordsRemover()\n",
    "* NGram()\n",
    "* CountVectorizer()\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame([ ['''Hello, guys! Look at the trees.''',], ['''He said, \"I'm pretty Good\".'''] ], [\"text\"])\n",
    "\n",
    "reTokenizer = ft.RegexTokenizer(inputCol=df.columns[0], outputCol='tokens', pattern='\\s+|[,.!?\\\"]')\n",
    "df1 = reTokenizer.transform(df)\n",
    "df1.show(truncate=False)\n",
    "+-------------------------------+-----------------------------------+\n",
    "|text                           |tokens                             |\n",
    "+-------------------------------+-----------------------------------+\n",
    "|Hello, guys! Look at the trees.|[hello, guys, look, at, the, trees]|\n",
    "|He said, \"I'm pretty Good\".    |[he, said, i'm, pretty, good]      |\n",
    "+-------------------------------+-----------------------------------+\n",
    "                            \n",
    "remover = ft.StopWordsRemover(inputCol=reTokenizer.getOutputCol(), outputCol='stop_removed')\n",
    "df2 = remover.transform(df1)\n",
    "df2.show(truncate=False)\n",
    "+-------------------------------+-----------------------------------+--------------------------+\n",
    "|text                           |tokens                             |stop_removed              |\n",
    "+-------------------------------+-----------------------------------+--------------------------+\n",
    "|Hello, guys! Look at the trees.|[hello, guys, look, at, the, trees]|[hello, guys, look, trees]|\n",
    "|He said, \"I'm pretty Good\".    |[he, said, i'm, pretty, good]      |[said, pretty, good]      |\n",
    "+-------------------------------+-----------------------------------+--------------------------+\n",
    "                            \n",
    "nGram = ft.NGram(n=2, inputCol=remover.getOutputCol(), outputCol=\"nGrams\")\n",
    "df3 = nGram.transform(df2)\n",
    "df3.select('nGrams').show(truncate=False)\n",
    "+-----------------------------------+\n",
    "|nGrams                             |\n",
    "+-----------------------------------+\n",
    "|[hello guys, guys look, look trees]|\n",
    "|[said pretty, pretty good]         |\n",
    "+-----------------------------------+\n",
    "                                  \n",
    "# Using a pipeline:\n",
    "from pyspark.ml import Pipeline\n",
    "                                  \n",
    "pipeline = Pipeline(stages=[reTokenizer, remover, nGram])\n",
    "pipeline.fit(df).transform(df).select('nGrams').show(truncate=False)\n",
    "                                  \n",
    "                                  \n",
    "indexer = ft.CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"text_indexed\")\n",
    "indexer.fit(df2).transform(df2).select('text_indexed').show(truncate=False)\n",
    "+-------------------------------+\n",
    "|text_indexed                   |\n",
    "+-------------------------------+   # There are 7 distinct words.\n",
    "|(7,[1,3,4,5],[1.0,1.0,1.0,1.0])|   # sparse vector corresponding to (0,1,0,1,1,1,0)\n",
    "|(7,[0,2,6],[1.0,1.0,1.0])      |   # sparse vector corresponding to (1,0,1,0,0,0,1)\n",
    "+-------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.ml.tuning\n",
    "\n",
    "### ParamGridBuilder(), CrossValidator(), TrainValidationSplit()\n",
    "\n",
    "```python\n",
    "lr = LogisticRegression()\n",
    "\n",
    "grid = ParamGridBuilder() \\\n",
    "     .baseOn({lr.labelCol: 'l'}) \\\n",
    "     .baseOn([lr.predictionCol, 'p']) \\\n",
    "     .addGrid(lr.regParam, [1.0, 2.0]) \\\n",
    "     .addGrid(lr.maxIter, [1, 5]) \\\n",
    "     .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "\n",
    "tvs = TrainValidationSplit(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)\n",
    "\n",
    "```\n",
    "\n",
    "TrainValidationSplit() evaluates each combination of parameters once, as opposed to $k$ times in the case of CrossValidator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### pyspark.ml.classification\n",
    "\n",
    "* LogisticRegression\n",
    "\n",
    "* DecisionTreeClassifier, RandomForestClassifier\n",
    "\n",
    "* GBTClassifier\n",
    "\n",
    "* NaiveBayes\n",
    "\n",
    "* MultilayerPerceptronClassifier\n",
    "\n",
    "* OneVsRest\n",
    "\n",
    "\n",
    "### pyspark.ml.regression\n",
    "\n",
    "* AFTSurvivalRegression\n",
    "\n",
    "* DecisionTreeRegressor\n",
    "\n",
    "* GBTRegressor\n",
    "\n",
    "* GeneralizedLinearRegression\n",
    "\n",
    "* IsotonicRegression\n",
    "\n",
    "* LinearRegression\n",
    "\n",
    "* RandomForestRegressor\n",
    "\n",
    "\n",
    "### pyspark.ml.clustering\n",
    "\n",
    "* KMeans, BisectingKMeans\n",
    "\n",
    "* GaussianMixture\n",
    "\n",
    "* LDA\n",
    "\n",
    "    Latent Dirichlet Allocation (LDA) is a topic model designed for text documents.\n",
    "\n",
    "```python\n",
    "# Assume data is a DataFrame consisting of strings (documents) and data.columns is ['documents'].\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    ft.RegexTokenizer(inputCol='documents', outputCol='tokens', pattern='\\s+|[,.?!\\\"]'),\n",
    "    ft.StopWordsRemover(inputCol='tokens', outputCol='tokens_stopRemoved'),\n",
    "    ft.CountVectorizer(inputCol='tokens_stopRemoved', outputCol='tokens_indexed'),\n",
    "    LDA(k=5, optimizer='online', featuresCol='tokens_indexed')])\n",
    "\n",
    "results = pipeline.fit(data).transform(data)\n",
    "results.columns\n",
    "['documents', 'tokens', 'tokens_stopRemoved', 'tokens_indexed', 'topicDistribution']\n",
    "\n",
    "# The rows in column 'topicDistribution' shows the probabilities of clusters. \n",
    "# To see the cluster numbers:\n",
    "results.rdd.map(lambda row: row[0].argmax()).collect()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
