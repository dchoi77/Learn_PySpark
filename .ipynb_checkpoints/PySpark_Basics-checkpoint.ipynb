{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips\n",
    "\n",
    "## Running PySpark in Jupyter\n",
    "\n",
    "```python\n",
    "# In Jupyter:\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"my_app\").getOrCreate()\n",
    "# Note: \n",
    "#   SparkSession creates a SparkContext object under the hood.\n",
    "#   Use spark.SparkContext to get the SparkContext object.\n",
    "#   spark.stop() terminates the SparkContext object.\n",
    "\n",
    "# df = spark.read.text('filename')\n",
    "# ...\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets\n",
    "\n",
    "```python\n",
    "filepath = \"examples/src/main/resources/people.txt\"\n",
    "data = sc.textFile(filepath)\n",
    "data.collect()        # ['Michael, 29', 'Andy, 30', 'Justin, 19']\n",
    "\n",
    "data2 = sc.parallelize([('Michael',29), ('Andy',30), ('Justin',19)])\n",
    "data2.first()         # ('Michael', 29)\n",
    "data2.take(2)         # [('Michael', 29), ('Andy', 30)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "\n",
    "```python\n",
    "data2.collect()     # [('Michael', 29), ('Andy', 30), ('Justin', 19)]\n",
    "```\n",
    "\n",
    "## map()\n",
    "\n",
    "```python\n",
    "data2.map(lambda row: len(row[0])+row[1]).collect()              # [36, 34, 25]\n",
    "data2.map(lambda row: (len(row[0]), row[1] % 10)).collect()      # [(7, 9), (4, 0), (6, 9)]\n",
    "```\n",
    "\n",
    "## flatMap()\n",
    "\n",
    "Similar to map(), but returns a flattened result.\n",
    "\n",
    "```python\n",
    "data2.flatMap(lambda row: (len(row[0]), row[1] % 10)).collect()  # [7, 9, 4, 0, 6, 9]\n",
    "```\n",
    "\n",
    "## filter()\n",
    "\n",
    "```python\n",
    "data2.filter(lambda row: row[1] % 2 ==1).count()      # 2\n",
    "```\n",
    "\n",
    "## groupBy()\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "rdd.groupBy(lambda x: x % 2).map(lambda x: (x[0], len(x[1]), sum(x[1]))).collect()\n",
    "[(0, 2, 10), (1, 4, 10)]\n",
    "\n",
    "\n",
    "rdd.collect()\n",
    "[[1, 1], [3, 3], [2, 0], [3, 3], [3, 0], [2, 1], [0, 3], [3, 3], [1, 3], [1, 0]]\n",
    "\n",
    "rdd.groupBy(lambda r: sum(r)).map(lambda x: (x[0], len(x[1]))).collect()\n",
    "[(1, 1), (2, 2), (3, 3), (4, 1), (6, 3)]\n",
    "```\n",
    "\n",
    "## distinct()\n",
    "\n",
    "```python\n",
    "sc.parallelize([2,4,2,3,1,4]).distinct().collect()   # [1,2,3,4]\n",
    "```\n",
    "\n",
    "## sample()\n",
    "\n",
    "sample(withReplacement, fraction, seed=None)\n",
    "\n",
    "```python\n",
    "sc.parallelize(range(100)).sample(True, 0.2, 4012).collect()\n",
    "# [7, 10, 11, 18, 22, 22, ..., 71, 71, 81, 90, 99]\n",
    "```\n",
    "\n",
    "## join(), leftOuterJoin(), intersection()\n",
    "\n",
    "```python\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "\n",
    "x.join(y).collect()            # [('a', (1, 2))]\n",
    "x.leftOuterJoin(y).collect()   # [('b', (4, None)), ('a', (1, 2))]\n",
    "x.intersection(y)              # []\n",
    "```\n",
    "\n",
    "## glom(), repartition()\n",
    "\n",
    "* glom() returns an RDD created by coalescing all elements within each partition into a list.\n",
    "\n",
    "* repartition() returns a new RDD that has exactly numPartitions partitions.\n",
    "\n",
    "```python\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    "\n",
    "rdd.glom().collect()                    # [[1], [2, 3], [4, 5], [6, 7]]\n",
    "rdd.repartition(2).glom().collect()     # [[1, 4, 5, 6, 7], [2, 3]]\n",
    "\n",
    "rdd.partitionBy(3)                      # Error\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions\n",
    "\n",
    "\n",
    "## take(), takeSample(), collect()\n",
    "\n",
    "* take(num)\n",
    "* takeSample(withReplacement, num, seed=None)\n",
    "\n",
    "\n",
    "## reduce(), reduceByKey()\n",
    "\n",
    "* reduce(f), where f is a commutative and associative binary operator\n",
    "\n",
    "```python\n",
    "from operator import add\n",
    "\n",
    "sc.parallelize([1, 2, 3, 4, 5]).reduce(add)  # or use .reduce(lambda a,b: a+b)\n",
    "\n",
    "sc.parallelize([(\"a\", 1), (\"b\", 3), (\"a\", 5)]).reduceByKey(add).collect()   # [('a', 6), ('b', 3)]\n",
    "\n",
    "sc.parallelize([]).reduce(add)               # Error\n",
    "```\n",
    "\n",
    "## count(), countByKey(), countByValue()\n",
    "\n",
    "To count the number of elements in an RDD, the driver does not need to collect the whole dataset. Don't use len(rnn.collect()). Instead, use count():\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "\n",
    "rdd.count()                         # 3\n",
    "rdd.countByKey().items()            # dict_items([('a', 2), ('b', 1)])\n",
    "rdd.countByValue().items()          # dict_items([(('a', 1), 2), (('b', 1), 1)])\n",
    "\n",
    "sc.parallelize([1,2,1,2,2]).countByValue().items()    # dict_items([(1, 2), (2, 3)])\n",
    "```\n",
    "\n",
    "## foreach()\n",
    "\n",
    "```python\n",
    "total = sc.accumulator(0)\n",
    "rdd = sc.parallelize(range(1,10))\n",
    "rdd.foreach(lambda x: total.add(x))\n",
    "total.value  # 45\n",
    "```\n",
    "Note that foreach() is not a transformation. The following does nothing.\n",
    "\n",
    "```python\n",
    "rdd.foreach(lambda x: x+5)\n",
    "```\n",
    "\n",
    "## cache(), persist(), unpersist()\n",
    "\n",
    "Spark automatically monitors cache usage on each node and drops out old data partitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "\n",
    "Similar to pandas dataframe and SQL table.\n",
    "\n",
    "Using RDDs in PySpark occurs a possibly large overhead between Python and the JVM.\n",
    "\n",
    "Using DataFrames, PySpark is often significantly fast.\n",
    "\n",
    "```python\n",
    "df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "type(df)\n",
    "<class 'pyspark.sql.dataframe.DataFrame'>\n",
    "\n",
    "df.show()\n",
    "+----+-------+\n",
    "| age|   name|\n",
    "+----+-------+\n",
    "|null|Michael|\n",
    "|  30|   Andy|\n",
    "|  19| Justin|\n",
    "+----+-------+\n",
    "\n",
    "type(df['age'])     # or use df.age\n",
    "<class 'pyspark.sql.column.Column'>\n",
    "\n",
    "type(df.select('age'))\n",
    "<class 'pyspark.sql.dataframe.DataFrame'>\n",
    "```\n",
    "\n",
    "## spark.createDataFrame()\n",
    "\n",
    "```python\n",
    "spark.createDataFrame([ (1,2) ], ['x', 'y']).show()\n",
    "+---+---+\n",
    "|  x|  y|\n",
    "+---+---+\n",
    "|  1|  2|\n",
    "+---+---+\n",
    "\n",
    "spark.createDataFrame([ (1,), (2,) ], ['x']).show()\n",
    "+---+\n",
    "|  x|\n",
    "+---+\n",
    "|  1|\n",
    "|  2|\n",
    "+---+\n",
    "\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType\n",
    "data = np.random.rand(10).round(2)\n",
    "schema = StructType([StructField('value', DoubleType(), False)])\n",
    "spark.createDataFrame([(float(x),) for x in data], schema=schema).show(n=3)\n",
    "+-----+\n",
    "|value|\n",
    "+-----+\n",
    "| 0.33|\n",
    "| 0.07|\n",
    "| 0.54|\n",
    "+-----+\n",
    "```\n",
    "\n",
    "## Methods\n",
    "\n",
    "### show(), printSchema()\n",
    "\n",
    "show(n=20, truncate=True, vertical=False)\n",
    "\n",
    "* n: number of rows to show\n",
    "* truncate=True truncates strings longer than 20 chars by default.\n",
    "* truncate=num truncates long strings to num\n",
    "\n",
    "```python\n",
    "df.show()\n",
    "+----+-------+\n",
    "| age|   name|\n",
    "+----+-------+\n",
    "|null|Michael|\n",
    "|  30|   Andy|\n",
    "|  19| Justin|\n",
    "+----+-------+\n",
    "```\n",
    "\n",
    "printSchema() prints out the schema in the tree format.\n",
    "\n",
    "```python\n",
    "df.printSchema()\n",
    "root\n",
    " |-- age: long (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    "```\n",
    "\n",
    "\n",
    "### collect(), take(), limit()\n",
    "\n",
    "collect() and take() returns a list of Row objects.\n",
    "\n",
    "limit() returns a DataFrame.\n",
    "\n",
    "```python\n",
    "df.collect()\n",
    "[Row(age=None, name='Michael'), Row(age=30, name='Andy'), Row(age=19, name='Justin')]\n",
    "\n",
    "df.take(2)\n",
    "[Row(age=None, name='Michael'), Row(age=30, name='Andy')]\n",
    "\n",
    "df.limit(2).show()\n",
    "+----+-------+\n",
    "| age|   name|\n",
    "+----+-------+\n",
    "|null|Michael|\n",
    "|  30|   Andy|\n",
    "+----+-------+\n",
    "```\n",
    "\n",
    "\n",
    "### distinct(), count()\n",
    "\n",
    "distinct() returns a new DataFrame containing the distinct rows\n",
    "\n",
    "### corr()\n",
    "\n",
    "```python\n",
    "df.corr('col1', 'col2')\n",
    "```\n",
    "\n",
    "### approxQuantile(): \n",
    "\n",
    "approxQuantile(col, probabilities, relativeError) calculates the approximate quantiles of numerical columns.\n",
    "\n",
    "```python\n",
    "df.approxQuantile('income', [0.25,0.5,0.75], 0.05)     # returns [Q1, median, Q3]\n",
    "```\n",
    "\n",
    "### describe()\n",
    "\n",
    "describe(*cols) computes basic statistics for numeric and string columns.\n",
    "\n",
    "```python\n",
    "df.describe(['col1', 'col2']).show()\n",
    "```\n",
    "\n",
    "### select(), selectExpr()\n",
    "\n",
    "select(*cols) projects a set of expressions and returns a new DataFrame.\n",
    "\n",
    "```python\n",
    "df.select('age').collect()\n",
    "[Row(age=None), Row(age=30), Row(age=19)]\n",
    "\n",
    "df.select((df.age+10).alias('future_age')).collect()\n",
    "[Row(future_age=None), Row(future_age=40), Row(future_age=29)]\n",
    "\n",
    "\n",
    "df.show()\n",
    "+---+---+---+---+\n",
    "| id|  x|  y|  z|\n",
    "+---+---+---+---+\n",
    "|  1|0.4|1.2|2.4|\n",
    "|  2|1.4|0.8|1.6|\n",
    "|  3|0.7|2.2|0.9|\n",
    "+---+---+---+---+\n",
    "\n",
    "df.select(*['id']+[((df[c] > 2.0) | (df[c] < 0.5)).alias(c+'_outliers') for c in ['x','y','z']]).show()\n",
    "# or use df.select('id', *[...]).show()\n",
    "+---+----------+----------+----------+\n",
    "| id|x_outliers|y_outliers|z_outliers|\n",
    "+---+----------+----------+----------+\n",
    "|  1|      true|     false|      true|\n",
    "|  2|     false|     false|     false|\n",
    "|  3|     false|      true|     false|\n",
    "+---+----------+----------+----------+\n",
    "```\n",
    "\n",
    "selectExpr() is a variant of select() that accepts SQL expressions.\n",
    "\n",
    "```python\n",
    "df.selectExpr(\"age * 2\", \"abs(age)\")\n",
    "\n",
    "df.selectExpr(\"age\", \"double(height)/double(weight) as ratio\")\n",
    "```\n",
    "\n",
    "### filter(), where()\n",
    "\n",
    "where() is an alias for filter().\n",
    "\n",
    "```python\n",
    "df.select(df.age, df.name).filter(df.age > 16).show()     # same as filter(\"age > 16\")\n",
    "+---+------+\n",
    "|age|  name|\n",
    "+---+------+\n",
    "| 30|  Andy|\n",
    "| 19|Justin|\n",
    "+---+------+\n",
    "\n",
    "df.filter(\"name like 'A%'\").show()\n",
    "+---+----+\n",
    "|age|name|\n",
    "+---+----+\n",
    "| 30|Andy|\n",
    "+---+----+\n",
    "\n",
    "\n",
    "df.filter(\"col1 = 'apple' AND col2 > 0\").groupBy(\"col3\", \"col4\").avg(\"col5\")\n",
    "```\n",
    "\n",
    "### withColumn()\n",
    "\n",
    "withColumn(colName, col) returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "\n",
    "```python\n",
    "df.withColumn('age2', df.age + 2).collect()\n",
    "[Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
    "```\n",
    "\n",
    "### sampleBy()\n",
    "\n",
    "sampleBy(col, fractions, seed=None)\n",
    "\n",
    "fractions: sampling fraction for each stratum. If a stratum is not specified, we treat its fraction as zero.\n",
    "\n",
    "```python\n",
    "df.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
    "```\n",
    "\n",
    "### groupBy(), groupby(), pivot()\n",
    "\n",
    "groupby is an alias for groupBy.\n",
    "\n",
    "Using groupBy() with no input makes each row a single group. By doing so, we can use aggregate functions. The following are all same:\n",
    "\n",
    "```python\n",
    "df.groupBy().max(\"amount\")\n",
    "df.agg({\"amount\": \"max\"})\n",
    "df.agg(F.max(df.amount))\n",
    "```\n",
    "\n",
    "pivot(pivot_col, values=None) pivots a column of the current DataFrame and perform the specified aggregation.\n",
    "\n",
    "```python\n",
    "# Compute the sum of earnings for each year by course with each course as a separate column\n",
    "\n",
    "df.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").collect()\n",
    "[Row(year=2012, dotNET=15000, Java=20000), Row(year=2013, dotNET=48000, Java=30000)]\n",
    "```\n",
    "\n",
    "In the above, we may not specify the values of \"course\" in pivot(), but it is less efficient, because Spark needs to first compute the list of distinct values internally.\n",
    "\n",
    "### join()\n",
    "\n",
    "```python\n",
    "df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)\n",
    "df.join(df2, 'name').select(df.name, df2.height)\n",
    "\n",
    "cond = [df.name == df3.name, df.age == df3.age]\n",
    "df.join(df3, cond, 'outer').select(df.name, df3.age)\n",
    "df.join(df4, ['name', 'age']).select(df.name, df.age)\n",
    "```\n",
    "\n",
    "### sort(), orderBy()\n",
    "\n",
    "```python\n",
    "df.sort(df.age.desc())\n",
    "df.sort(\"age\", ascending=False)\n",
    "df.sort(desc(\"age\"))\n",
    "\n",
    "df.orderBy(df.age.desc())\n",
    "df.orderBy(desc(\"age\"), \"name\")\n",
    "df.orderBy([\"age\", \"name\"], ascending=[0, 1])\n",
    "```    \n",
    "\n",
    "### drop(), dropDuplicates(), drop_duplicates()\n",
    "\n",
    "```python\n",
    "df.drop('age')\n",
    "df.join(df2, df.name == df2.name, 'inner').drop(df.name)\n",
    "\n",
    "df.dropDuplicates()\n",
    "df.dropDuplicates(['name', 'height'])\n",
    "```\n",
    "\n",
    "### agg()\n",
    "\n",
    "df.agg() is a shorthand for df.groupBy.agg().\n",
    "\n",
    "```python\n",
    "df.agg({\"age\": \"max\", \"weight\": \"skewness\"}).collect()\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.agg(F.max(df.age)).collect()\n",
    "\n",
    "\n",
    "df.show()\n",
    "+-----+------+\n",
    "|first|second|\n",
    "+-----+------+\n",
    "|  2.0|   4.2|\n",
    "|  3.5|   2.8|\n",
    "+-----+------+\n",
    "\n",
    "df.agg(*[( (F.max(c) - F.min(c))/F.stddev(c) ).alias(c+'_transformed') for c in df.columns]).show()\n",
    "+------------------+------------------+\n",
    "| first_transformed|second_transformed|\n",
    "+------------------+------------------+\n",
    "|1.4142135623730951| 1.414213562373095|\n",
    "+------------------+------------------+\n",
    "```\n",
    "\n",
    "### toPandas()\n",
    "\n",
    "df.toPandas() is a pandas dataframe.\n",
    "\n",
    "\n",
    "## Column operations\n",
    "\n",
    "```python\n",
    "df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "df.columns\n",
    "['age', 'name']\n",
    "\n",
    "type(df.age)\n",
    "<class 'pyspark.sql.column.Column'>\n",
    "```\n",
    "\n",
    "### alias()\n",
    "\n",
    "```python\n",
    "df.select(df.age.alias('Age')).columns\n",
    "['Age']\n",
    "```\n",
    "\n",
    "### cast(), astype()\n",
    "\n",
    "```python\n",
    "df.dtypes[0]\n",
    "('age', 'bigint')\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn('age', df.age.cast(IntegerType()))\n",
    "df.dtypes[0]\n",
    "('age', 'int')\n",
    "```\n",
    "\n",
    "### isNull(), isNotNull()\n",
    "\n",
    "```python\n",
    "df.filter(df.height.isNull())\n",
    "```\n",
    "\n",
    "### isin(), between()\n",
    "\n",
    "```python\n",
    "df[df.name.isin(\"Mike\",\"Michael\")].show()\n",
    "\n",
    "df.select(df.name, df.age.between(2, 4)).show()\n",
    "```\n",
    "\n",
    "### startswith(), endswith()\n",
    "\n",
    "```python\n",
    "df.filter(df.name.startswith('Al')).collect()\n",
    "```\n",
    "\n",
    "### substr()\n",
    "\n",
    "substr(startPos, length)\n",
    "\n",
    "```python\n",
    " df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
    "[Row(col='Mic'), Row(col='And'), Row(col='Jus')]\n",
    "```\n",
    "\n",
    "### when(), otherwise()\n",
    "\n",
    "```python\n",
    "df.show()\n",
    "+----+-------+\n",
    "| age|   name|\n",
    "+----+-------+\n",
    "|null|Michael|\n",
    "|  30|   Andy|\n",
    "|  19| Justin|\n",
    "+----+-------+\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "df.select(df.name, F.when(df.age > 25, 2).when(df.age > 15, 1).otherwise(0).alias('age_gp')).show()\n",
    "+-------+------+\n",
    "|   name|age_gp|\n",
    "+-------+------+\n",
    "|Michael|     0|\n",
    "|   Andy|     2|\n",
    "| Justin|     1|\n",
    "+-------+------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View & SQL query\n",
    "\n",
    "### Global temporary view\n",
    "\n",
    "A global temporary view can be shared among all sesessions.\n",
    "\n",
    "```python\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "# Use global_temp.name:\n",
    "spark.sql(\"SELECT name, age FROM global_temp.people WHERE age IS NOT NULL\").show()\n",
    "+------+---+\n",
    "|  name|age|\n",
    "+------+---+\n",
    "|  Andy| 30|\n",
    "|Justin| 19|\n",
    "+------+---+\n",
    "\n",
    "spark.newSession().sql(\"SELECT name, age FROM global_temp.people WHERE age IS NOT NULL\").show()\n",
    "# the same result as above\n",
    "```\n",
    "\n",
    "### Temporary view\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "spark.sql(\"SELECT name, age FROM people WHERE age IS NOT NULL\").show()\n",
    "# the same result as above\n",
    "\n",
    "spark.newSession().sql(\"SELECT name, age FROM people WHERE age IS NOT NULL\").show()\n",
    "# Error\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD to DataFrame\n",
    "\n",
    "### Inferring the schema using reflection\n",
    "\n",
    "```python\n",
    "rdd = sc.textFile(\"examples/src/main/resources/people.json\")    # RDD\n",
    "df = spark.read.json(rdd)                                       # DataFrame\n",
    "\n",
    "df.printSchema()\n",
    "root\n",
    " |-- age: long (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    "    \n",
    "    \n",
    "from pyspark.sql import Row\n",
    "rdd = sc.parallelize([Row(name='Michael',age=None), Row(name='Andy', age=30), Row(name='Justin', age=19)])\n",
    "df = rdd.toDF()\n",
    "df.show()\n",
    "+-------+----+\n",
    "|   name| age|\n",
    "+-------+----+\n",
    "|Michael|null|\n",
    "|   Andy|  30|\n",
    "| Justin|  19|\n",
    "+-------+----+\n",
    "```\n",
    "\n",
    "### Specifying the schema\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "rdd = sc.textFile(\"examples/src/main/resources/people.txt\") \n",
    "rdd.collect()\n",
    "['Michael, 29', 'Andy, 30', 'Justin, 19']\n",
    "\n",
    "rdd = rdd.map(lambda x: x.split(',')).map(lambda x: (x[0], int(x[1])))\n",
    "rdd.collect()\n",
    "[('Michael', 29), ('Andy', 30), ('Justin', 19)]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", LongType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.printSchema()\n",
    "root\n",
    " |-- name: string (nullable = false)\n",
    " |-- age: long (nullable = true)\n",
    "    \n",
    "df.createOrReplaceTempView(\"people\")\n",
    "spark.sql(\"SELECT * FROM people WHERE age > 20\").show()\n",
    "+-------+---+\n",
    "|   name|age|\n",
    "+-------+---+\n",
    "|Michael| 29|\n",
    "|   Andy| 30|\n",
    "+-------+---+\n",
    "```\n",
    "\n",
    "\n",
    "Another example:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType\n",
    "\n",
    "rdd = sc.parallelize(['\"x\",\"y\",\"z\"', '2,3,1', '3,5,2', '8,3,4'])\n",
    "header = rdd.first()\n",
    "rdd = rdd.filter(lambda r: r != header).map(lambda r: r.split(',')).map(lambda r: [int(x) for x in r])\n",
    "schema = StructType([StructField(c[1:-1], IntegerType(), True) for c in header.split(',')])\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.show()\n",
    "+---+---+---+\n",
    "|  x|  y|  z|\n",
    "+---+---+---+\n",
    "|  2|  3|  1|\n",
    "|  3|  5|  2|\n",
    "|  8|  3|  4|\n",
    "+---+---+---+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "\n",
    "## dropDuplicates()\n",
    "\n",
    "dropDuplicates(subset=None)\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame([\n",
    "   (1, 10.2, 'a'),\n",
    "   (2, 15.8, 'b'),\n",
    "   (3, 4.5, None),\n",
    "   (2, 15.8, 'b'),\n",
    "   (3, 10.2, 'a'),\n",
    "   (1, 18.3, 'b')], ['id', 'score', 'category'])\n",
    "\n",
    "# Distinct rows:\n",
    "if df.count() != df.distinct().count(): \n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "# Distinct (score, category):\n",
    "subset = [c for c in df.columns if c != 'id']\n",
    "\n",
    "if df.count() != df.select(subset).distinct().count():\n",
    "    df = df.dropDuplicates(subset)\n",
    "\n",
    "df.show()\n",
    "+---+-----+--------+\n",
    "| id|score|category|\n",
    "+---+-----+--------+\n",
    "|  1| 18.3|       b|\n",
    "|  3|  4.5|    null|\n",
    "|  1| 10.2|       a|\n",
    "|  2| 15.8|       b|\n",
    "+---+-----+--------+ \n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "df = df.withColumn('unique_id', F.monotonically_increasing_id())\n",
    "df.show()\n",
    "+---+-----+--------+-------------+\n",
    "| id|score|category|    unique_id|\n",
    "+---+-----+--------+-------------+\n",
    "|  1| 18.3|       b| 231928233984|\n",
    "|  3|  4.5|    null| 231928233985|\n",
    "|  1| 10.2|       a|1099511627776|\n",
    "|  2| 15.8|       b|1348619730944|\n",
    "+---+-----+--------+-------------+\n",
    "```\n",
    "\n",
    "## na.drop(), na.fill()\n",
    "\n",
    "* dropna() or na.drop(): how='any', thresh=None, subset=None\n",
    "\n",
    "* fillna() or na.fill(): value, subset=None\n",
    "\n",
    "fillna(value, subset=None), alias for na.fill()\n",
    "    \n",
    "```mysql\n",
    "df.na.fill({'age': 50, 'name': 'unknown'})\n",
    "```\n",
    "\n",
    "* Find the number of missing values in each row:\n",
    "\n",
    "```mysql\n",
    "df.rdd.map(lambda r: sum([c == None for c in r])).collect()\n",
    "```\n",
    "\n",
    "## outliers\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.range(1,11) \\\n",
    "  .withColumn('x', F.round(10*F.rand(),2)) \\\n",
    "  .withColumn('y', F.round(10*F.rand(),2))\n",
    "\n",
    "df.show()\n",
    "+---+----+----+\n",
    "| id|   x|   y|\n",
    "+---+----+----+\n",
    "|  1|3.81|9.86|\n",
    "|  2|0.56|7.07|\n",
    "| ...         |\n",
    "|  9|9.46|5.27|\n",
    "| 10| 4.5| 2.0|\n",
    "+---+----+----+\n",
    "\n",
    "bounds = dict()\n",
    "for c in ['x','y']:\n",
    "    bounds[c] = df.approxQuantile(c, [0.2,0.8], 0.05)\n",
    "bounds\n",
    "{'x': [0.56, 8.15], 'y': [2.0, 8.6]}\n",
    "\n",
    "is_outlier = df.select('id', *[ ( (df[c] < bounds[c][0]) | (df[c] > bounds[c][1]) ).alias(c + '_out') for c in ['x','y'] ])\n",
    "is_outlier.show()\n",
    "+---+-----+-----+\n",
    "| id|x_out|y_out|\n",
    "+---+-----+-----+\n",
    "|  1|false| true|\n",
    "|  2|false|false|\n",
    "| ...           |\n",
    "|  9| true|false|\n",
    "| 10|false|false|\n",
    "+---+-----+-----+\n",
    "\n",
    "\n",
    "df.join(is_outlier, on='id').filter(\"x_out = 'false'\").select('id','x').show()\n",
    "df.join(is_outlier, on='id').filter(\"y_out = 'false'\").select('id','y').show()\n",
    "df.join(is_outlier, on='id').filter(\"x_out = 'false' and y_out = 'false'\").select('id','x','y').show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark functions\n",
    "\n",
    "## spark.range()\n",
    "\n",
    "```python\n",
    "spark.range(1,6,2).show()\n",
    "+---+\n",
    "| id|\n",
    "+---+\n",
    "|  1|\n",
    "|  3|\n",
    "|  5|\n",
    "+---+\n",
    "```\n",
    "\n",
    "## pyspark.sql.functions\n",
    "\n",
    "pyspark.sql.functions is a collection of built-in functions.\n",
    "\n",
    "```python\n",
    "import pyspark.sql.functions as F\n",
    "```\n",
    "\n",
    "### F.when().otherwise() \n",
    "\n",
    "```python\n",
    "df.withColumn('score', F.when(F.col('score') > 80, F.col('score')).otherwise(0))\n",
    "```\n",
    "\n",
    "### F.udf()\n",
    "\n",
    "udf(f=None, returnType=StringType) creates a user defined function (UDF).\n",
    "\n",
    "The user-defined functions are considered deterministic by default. If your function is not deterministic, call `asNondeterministic` on the user defined function.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import IntegerType\n",
    "import random\n",
    "random_udf = F.udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
    "\n",
    "type(random_udf())\n",
    "<class 'pyspark.sql.column.Column'>\n",
    "\n",
    "random_udf.func()   # 52\n",
    "random_udf.func()   # 7\n",
    "\n",
    "\n",
    "slen = F.udf(lambda s: len(s), IntegerType())  # slen.func(\"John\") returns 4.\n",
    "\n",
    "@F.udf               # By default, returnType is StringType().\n",
    "def to_upper(s): \n",
    "    if s is not None: return s.upper()         # to_upper.func(\"John\") returns 'JOHN'.\n",
    "\n",
    "@F.udf(returnType=IntegerType())               \n",
    "def add_one(x):                                # add_one.func(9) returns 10.\n",
    "    if x is not None: return x + 1\n",
    "\n",
    "welcome = F.udf(lambda a, b: a +', ' + b)\n",
    "\n",
    "df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
    "\n",
    "df.select(slen(\"name\").alias(\"name_len\"), \n",
    "          to_upper(\"name\"), \n",
    "          add_one(\"age\").alias(\"age+1\"), \n",
    "          welcome(\"name\", F.lit(\"Good morning!\")).alias(\"message\")).show()\n",
    "+--------+--------------+-----+--------------------+\n",
    "|name_len|to_upper(name)|age+1|             message|\n",
    "+--------+--------------+-----+--------------------+\n",
    "|       8|      JOHN DOE|   22|John Doe, Good mo...|\n",
    "+--------+--------------+-----+--------------------+\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
