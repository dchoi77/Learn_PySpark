{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips\n",
    "\n",
    "## Running PySpark in Jupyter\n",
    "\n",
    "```python\n",
    "# In Jupyter:\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"my_app\").getOrCreate()\n",
    "# Note: \n",
    "#   SparkSession creates a SparkContext object under the hood.\n",
    "#   Use spark.SparkContext to get the SparkContext object.\n",
    "#   spark.stop() terminates the SparkContext object.\n",
    "\n",
    "# df = spark.read.text('filename')\n",
    "# ...\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets\n",
    "\n",
    "```python\n",
    "sc.textFile(filepath)\n",
    "sc.parallelize([('Michael',29), ('Andy',30), ('Justin',19)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods\n",
    "\n",
    "## collect(), take(), takeSample()\n",
    "\n",
    "```python\n",
    "data2.collect()     # [('Michael', 29), ('Andy', 30), ('Justin', 19)]\n",
    "```\n",
    "\n",
    "* take(num)\n",
    "\n",
    "* takeSample(withReplacement, num, seed=None)\n",
    "\n",
    "\n",
    "## map()\n",
    "\n",
    "```python\n",
    "data2.map(lambda row: len(row[0])+row[1]).collect()              # [36, 34, 25]\n",
    "data2.map(lambda row: (len(row[0]), row[1] % 10)).collect()      # [(7, 9), (4, 0), (6, 9)]\n",
    "```\n",
    "\n",
    "## flatMap()\n",
    "\n",
    "Similar to map(), but returns a flattened result.\n",
    "\n",
    "```python\n",
    "data2.flatMap(lambda row: (len(row[0]), row[1] % 10)).collect()  # [7, 9, 4, 0, 6, 9]\n",
    "```\n",
    "\n",
    "## filter()\n",
    "\n",
    "```python\n",
    "data2.filter(lambda row: row[1] % 2 ==1).count()      # 2\n",
    "```\n",
    "\n",
    "## groupBy()\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "rdd.groupBy(lambda x: x % 2).map(lambda x: (x[0], len(x[1]), sum(x[1]))).collect()\n",
    "[(0, 2, 10), (1, 4, 10)]\n",
    "\n",
    "\n",
    "rdd.collect()\n",
    "[[1, 1], [3, 3], [2, 0], [3, 3], [3, 0], [2, 1], [0, 3], [3, 3], [1, 3], [1, 0]]\n",
    "\n",
    "rdd.groupBy(lambda r: sum(r)).map(lambda x: (x[0], len(x[1]))).collect()\n",
    "[(1, 1), (2, 2), (3, 3), (4, 1), (6, 3)]\n",
    "```\n",
    "\n",
    "## distinct()\n",
    "\n",
    "```python\n",
    "sc.parallelize([2,4,2,3,1,4]).distinct().collect()   # [1,2,3,4]\n",
    "```\n",
    "\n",
    "## sample()\n",
    "\n",
    "sample(withReplacement, fraction, seed=None)\n",
    "\n",
    "```python\n",
    "sc.parallelize(range(100)).sample(True, 0.2, 4012).collect()\n",
    "# [7, 10, 11, 18, 22, 22, ..., 71, 71, 81, 90, 99]\n",
    "```\n",
    "\n",
    "## join(), leftOuterJoin(), intersection()\n",
    "\n",
    "```python\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "\n",
    "x.join(y).collect()            # [('a', (1, 2))]\n",
    "x.leftOuterJoin(y).collect()   # [('b', (4, None)), ('a', (1, 2))]\n",
    "x.intersection(y)              # []\n",
    "```\n",
    "\n",
    "## glom(), repartition()\n",
    "\n",
    "* glom() returns an RDD created by coalescing all elements within each partition into a list.\n",
    "\n",
    "* repartition() returns a new RDD that has exactly numPartitions partitions.\n",
    "\n",
    "```python\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    "\n",
    "rdd.glom().collect()                    # [[1], [2, 3], [4, 5], [6, 7]]\n",
    "rdd.repartition(2).glom().collect()     # [[1, 4, 5, 6, 7], [2, 3]]\n",
    "\n",
    "rdd.partitionBy(3)                      # Error\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce(), reduceByKey()\n",
    "\n",
    "* reduce(f), where f is a commutative and associative binary operator\n",
    "\n",
    "```python\n",
    "from operator import add\n",
    "\n",
    "sc.parallelize([1, 2, 3, 4, 5]).reduce(add)  # or use .reduce(lambda a,b: a+b)\n",
    "\n",
    "sc.parallelize([(\"a\", 1), (\"b\", 3), (\"a\", 5)]).reduceByKey(add).collect()   # [('a', 6), ('b', 3)]\n",
    "\n",
    "sc.parallelize([]).reduce(add)               # Error\n",
    "```\n",
    "\n",
    "## count(), countByKey(), countByValue()\n",
    "\n",
    "To count the number of elements in an RDD, the driver does not need to collect the whole dataset. Don't use len(rnn.collect()). Instead, use count():\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "\n",
    "rdd.count()                         # 3\n",
    "rdd.countByKey().items()            # dict_items([('a', 2), ('b', 1)])\n",
    "rdd.countByValue().items()          # dict_items([(('a', 1), 2), (('b', 1), 1)])\n",
    "\n",
    "sc.parallelize([1,2,1,2,2]).countByValue().items()    # dict_items([(1, 2), (2, 3)])\n",
    "```\n",
    "\n",
    "## foreach()\n",
    "\n",
    "```python\n",
    "total = sc.accumulator(0)\n",
    "rdd = sc.parallelize(range(1,10))\n",
    "rdd.foreach(lambda x: total.add(x))\n",
    "total.value  # 45\n",
    "```\n",
    "Note that foreach() is not a transformation. The following does nothing.\n",
    "\n",
    "```python\n",
    "rdd.foreach(lambda x: x+5)\n",
    "```\n",
    "\n",
    "## cache(), persist(), unpersist()\n",
    "\n",
    "Spark automatically monitors cache usage on each node and drops out old data partitions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
