{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips\n",
    "\n",
    "## Running PySpark in Jupyter\n",
    "\n",
    "```python\n",
    "# In Jupyter:\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"my_app\").getOrCreate()\n",
    "# Note: \n",
    "#   SparkSession creates a SparkContext object under the hood.\n",
    "#   Use spark.SparkContext to get the SparkContext object.\n",
    "#   spark.stop() terminates the SparkContext object.\n",
    "\n",
    "# df = spark.read.text('filename')\n",
    "# ...\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets\n",
    "\n",
    "```python\n",
    "filepath = \"examples/src/main/resources/people.txt\"\n",
    "data = sc.textFile(filepath)\n",
    "data.collect()        # ['Michael, 29', 'Andy, 30', 'Justin, 19']\n",
    "\n",
    "data2 = sc.parallelize([('Michael',29), ('Andy',30), ('Justin',19)])\n",
    "data2.first()         # ('Michael', 29)\n",
    "data2.take(2)         # [('Michael', 29), ('Andy', 30)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "\n",
    "```python\n",
    "data2.collect()     # [('Michael', 29), ('Andy', 30), ('Justin', 19)]\n",
    "```\n",
    "\n",
    "## map()\n",
    "\n",
    "```python\n",
    "data2.map(lambda row: len(row[0])+row[1]).collect()              # [36, 34, 25]\n",
    "data2.map(lambda row: (len(row[0]), row[1] % 10)).collect()      # [(7, 9), (4, 0), (6, 9)]\n",
    "```\n",
    "\n",
    "## flatMap()\n",
    "\n",
    "Similar to map(), but returns a flattened result.\n",
    "\n",
    "```python\n",
    "data2.flatMap(lambda row: (len(row[0]), row[1] % 10)).collect()  # [7, 9, 4, 0, 6, 9]\n",
    "```\n",
    "\n",
    "## filter()\n",
    "\n",
    "```python\n",
    "data2.filter(lambda row: row[1] % 2 ==1).count()      # 2\n",
    "```\n",
    "\n",
    "## groupBy()\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
    "rdd.groupBy(lambda x: x % 2).map(lambda x: (x[0], len(x[1]), sum(x[1]))).collect()\n",
    "[(0, 2, 10), (1, 4, 10)]\n",
    "\n",
    "\n",
    "rdd.collect()\n",
    "[[1, 1], [3, 3], [2, 0], [3, 3], [3, 0], [2, 1], [0, 3], [3, 3], [1, 3], [1, 0]]\n",
    "\n",
    "rdd.groupBy(lambda r: sum(r)).map(lambda x: (x[0], len(x[1]))).collect()\n",
    "[(1, 1), (2, 2), (3, 3), (4, 1), (6, 3)]\n",
    "```\n",
    "\n",
    "## distinct()\n",
    "\n",
    "```python\n",
    "sc.parallelize([2,4,2,3,1,4]).distinct().collect()   # [1,2,3,4]\n",
    "```\n",
    "\n",
    "## sample()\n",
    "\n",
    "sample(withReplacement, fraction, seed=None)\n",
    "\n",
    "```python\n",
    "sc.parallelize(range(100)).sample(True, 0.2, 4012).collect()\n",
    "# [7, 10, 11, 18, 22, 22, ..., 71, 71, 81, 90, 99]\n",
    "```\n",
    "\n",
    "## join(), leftOuterJoin(), intersection()\n",
    "\n",
    "```python\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2)])\n",
    "\n",
    "x.join(y).collect()            # [('a', (1, 2))]\n",
    "x.leftOuterJoin(y).collect()   # [('b', (4, None)), ('a', (1, 2))]\n",
    "x.intersection(y)              # []\n",
    "```\n",
    "\n",
    "## glom(), repartition()\n",
    "\n",
    "* glom() returns an RDD created by coalescing all elements within each partition into a list.\n",
    "\n",
    "* repartition() returns a new RDD that has exactly numPartitions partitions.\n",
    "\n",
    "```python\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
    "\n",
    "rdd.glom().collect()                    # [[1], [2, 3], [4, 5], [6, 7]]\n",
    "rdd.repartition(2).glom().collect()     # [[1, 4, 5, 6, 7], [2, 3]]\n",
    "\n",
    "rdd.partitionBy(3)                      # Error\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions\n",
    "\n",
    "\n",
    "## take(), takeSample(), collect()\n",
    "\n",
    "* take(num)\n",
    "* takeSample(withReplacement, num, seed=None)\n",
    "\n",
    "\n",
    "## reduce(), reduceByKey()\n",
    "\n",
    "* reduce(f), where f is a commutative and associative binary operator\n",
    "\n",
    "```python\n",
    "from operator import add\n",
    "\n",
    "sc.parallelize([1, 2, 3, 4, 5]).reduce(add)  # or use .reduce(lambda a,b: a+b)\n",
    "\n",
    "sc.parallelize([(\"a\", 1), (\"b\", 3), (\"a\", 5)]).reduceByKey(add).collect()   # [('a', 6), ('b', 3)]\n",
    "\n",
    "sc.parallelize([]).reduce(add)               # Error\n",
    "```\n",
    "\n",
    "## count(), countByKey(), countByValue()\n",
    "\n",
    "To count the number of elements in an RDD, the driver does not need to collect the whole dataset. Don't use len(rnn.collect()). Instead, use count():\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "\n",
    "rdd.count()                         # 3\n",
    "rdd.countByKey().items()            # dict_items([('a', 2), ('b', 1)])\n",
    "rdd.countByValue().items()          # dict_items([(('a', 1), 2), (('b', 1), 1)])\n",
    "\n",
    "sc.parallelize([1,2,1,2,2]).countByValue().items()    # dict_items([(1, 2), (2, 3)])\n",
    "```\n",
    "\n",
    "## foreach()\n",
    "\n",
    "```python\n",
    "total = sc.accumulator(0)\n",
    "rdd = sc.parallelize(range(1,10))\n",
    "rdd.foreach(lambda x: total.add(x))\n",
    "total.value  # 45\n",
    "```\n",
    "Note that foreach() is not a transformation. The following does nothing.\n",
    "\n",
    "```python\n",
    "rdd.foreach(lambda x: x+5)\n",
    "```\n",
    "\n",
    "## cache(), persist(), unpersist()\n",
    "\n",
    "Spark automatically monitors cache usage on each node and drops out old data partitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "\n",
    "Similar to pandas dataframe and SQL table.\n",
    "\n",
    "Using RDDs in PySpark occurs a possibly large overhead between Python and the JVM.\n",
    "\n",
    "Using DataFrames, PySpark is often significantly fast.\n",
    "\n",
    "```python\n",
    "df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "type(df)\n",
    "<class 'pyspark.sql.dataframe.DataFrame'>\n",
    "```\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "### show(), collect(), take(), printSchema()\n",
    "\n",
    "show(n=20, truncate=True, vertical=False)\n",
    "\n",
    "* n: number of rows to show\n",
    "* truncate=True truncates strings longer than 20 chars by default.\n",
    "* truncate=num truncates long strings to num\n",
    "\n",
    "```python\n",
    "df.show()\n",
    "+----+-------+\n",
    "| age|   name|\n",
    "+----+-------+\n",
    "|null|Michael|\n",
    "|  30|   Andy|\n",
    "|  19| Justin|\n",
    "+----+-------+\n",
    "```\n",
    "\n",
    "collect() returns all the records as a list of Row objects.\n",
    "\n",
    "```python\n",
    "df.collect()\n",
    "[Row(age=None, name='Michael'), Row(age=30, name='Andy'), Row(age=19, name='Justin')]\n",
    "\n",
    "df.take(2)\n",
    "[Row(age=None, name='Michael'), Row(age=30, name='Andy')]\n",
    "```\n",
    "\n",
    "printSchema() prints out the schema in the tree format.\n",
    "\n",
    "```python\n",
    "df.printSchema()\n",
    "root\n",
    " |-- age: long (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    "```\n",
    "\n",
    "### distinct(), count()\n",
    "\n",
    "distinct() returns a new DataFrame containing the distinct rows\n",
    "\n",
    "### corr()\n",
    "\n",
    "```python\n",
    "df.corr('col1', 'col2')\n",
    "```\n",
    "\n",
    "### approxQuantile(): \n",
    "\n",
    "approxQuantile(col, probabilities, relativeError) calculates the approximate quantiles of numerical columns.\n",
    "\n",
    "```python\n",
    "df.approxQuantile('income', [0.25,0.5,0.75], 0.05)     # returns [Q1, median, Q3]\n",
    "```\n",
    "\n",
    "### describe()\n",
    "\n",
    "describe(*cols) computes basic statistics for numeric and string columns.\n",
    "\n",
    "```python\n",
    "df.describe(['col1', 'col2']).show()\n",
    "```\n",
    "\n",
    "### select()\n",
    "\n",
    "select(*cols) projects a set of expressions and returns a new DataFrame.\n",
    "\n",
    "```python\n",
    "df.select('age').collect()\n",
    "[Row(age=None), Row(age=30), Row(age=19)]\n",
    "\n",
    "df.select((df.age+10).alias('future_age')).collect()\n",
    "[Row(future_age=None), Row(future_age=40), Row(future_age=29)]\n",
    "\n",
    "\n",
    "df.show()\n",
    "+---+---+---+---+\n",
    "| id|  x|  y|  z|\n",
    "+---+---+---+---+\n",
    "|  1|0.4|1.2|2.4|\n",
    "|  2|1.4|0.8|1.6|\n",
    "|  3|0.7|2.2|0.9|\n",
    "+---+---+---+---+\n",
    "\n",
    "df.select(*['id']+[((df[c] > 2.0) | (df[c] < 0.5)).alias(c+'_outliers') for c in ['x','y','z']]).show()\n",
    "# or use df.select('id', *[...]).show()\n",
    "+---+----------+----------+----------+\n",
    "| id|x_outliers|y_outliers|z_outliers|\n",
    "+---+----------+----------+----------+\n",
    "|  1|      true|     false|      true|\n",
    "|  2|     false|     false|     false|\n",
    "|  3|     false|      true|     false|\n",
    "+---+----------+----------+----------+\n",
    "```\n",
    "\n",
    "### filter(), where()\n",
    "\n",
    "where() is an alias for filter().\n",
    "\n",
    "```python\n",
    "df.select(df.age, df.name).filter(df.age > 16).show()     # same as filter(\"age > 16\")\n",
    "+---+------+\n",
    "|age|  name|\n",
    "+---+------+\n",
    "| 30|  Andy|\n",
    "| 19|Justin|\n",
    "+---+------+\n",
    "\n",
    "df.filter(\"name like 'A%'\").show()\n",
    "+---+----+\n",
    "|age|name|\n",
    "+---+----+\n",
    "| 30|Andy|\n",
    "+---+----+\n",
    "```\n",
    "\n",
    "### withColumn()\n",
    "\n",
    "withColumn(colName, col) returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "\n",
    "```python\n",
    "df.withColumn('age2', df.age + 2).collect()\n",
    "[Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
    "```\n",
    "\n",
    "### sampleBy()\n",
    "\n",
    "sampleBy(col, fractions, seed=None)\n",
    "\n",
    "fractions: sampling fraction for each stratum. If a stratum is not specified, we treat its fraction as zero.\n",
    "\n",
    "```python\n",
    "df.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
    "```\n",
    "\n",
    "### groupBy(), groupby(), pivot()\n",
    "\n",
    "groupby is an alias for groupBy.\n",
    "\n",
    "pivot(pivot_col, values=None) pivots a column of the current DataFrame and perform the specified aggregation.\n",
    "\n",
    "```python\n",
    "# Compute the sum of earnings for each year by course with each course as a separate column\n",
    "\n",
    "df.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").collect()\n",
    "[Row(year=2012, dotNET=15000, Java=20000), Row(year=2013, dotNET=48000, Java=30000)]\n",
    "```\n",
    "\n",
    "In the above, we may not specify the values of \"course\" in pivot(), but it is less efficient, because Spark needs to first compute the list of distinct values internally.\n",
    "\n",
    "### agg()\n",
    "\n",
    "df.agg() is a shorthand for df.groupBy.agg().\n",
    "\n",
    "```python\n",
    "df.agg({\"age\": \"max\", \"weight\": \"skewness\"}).collect()\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.agg(F.max(F.age)).collect()\n",
    "\n",
    "\n",
    "df.show()\n",
    "+-----+------+\n",
    "|first|second|\n",
    "+-----+------+\n",
    "|  2.0|   4.2|\n",
    "|  3.5|   2.8|\n",
    "+-----+------+\n",
    "\n",
    "df.agg(*[( (F.max(c) - F.min(c))/F.stddev(c) ).alias(c+'_transformed') for c in df.columns]).show()\n",
    "+------------------+------------------+\n",
    "| first_transformed|second_transformed|\n",
    "+------------------+------------------+\n",
    "|1.4142135623730951| 1.414213562373095|\n",
    "+------------------+------------------+\n",
    "```\n",
    "\n",
    "### toPandas()\n",
    "\n",
    "df.toPandas() is a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View & SQL query\n",
    "\n",
    "### Global temporary view\n",
    "\n",
    "A global temporary view can be shared among all sesessions.\n",
    "\n",
    "```python\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "# Use global_temp.name:\n",
    "spark.sql(\"SELECT name, age FROM global_temp.people WHERE age IS NOT NULL\").show()\n",
    "+------+---+\n",
    "|  name|age|\n",
    "+------+---+\n",
    "|  Andy| 30|\n",
    "|Justin| 19|\n",
    "+------+---+\n",
    "\n",
    "spark.newSession().sql(\"SELECT name, age FROM global_temp.people WHERE age IS NOT NULL\").show()\n",
    "# the same result as above\n",
    "```\n",
    "\n",
    "### Temporary view\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "spark.sql(\"SELECT name, age FROM people WHERE age IS NOT NULL\").show()\n",
    "# the same result as above\n",
    "\n",
    "spark.newSession().sql(\"SELECT name, age FROM people WHERE age IS NOT NULL\").show()\n",
    "# Error\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD to DataFrame\n",
    "\n",
    "### Inferring the schema using reflection\n",
    "\n",
    "```python\n",
    "rdd = sc.textFile(\"examples/src/main/resources/people.json\")    # RDD\n",
    "df = spark.read.json(rdd)                                       # DataFrame\n",
    "\n",
    "df.printSchema()\n",
    "root\n",
    " |-- age: long (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    "    \n",
    "    \n",
    "from pyspark.sql import Row\n",
    "rdd = sc.parallelize([Row(name='Michael',age=None), Row(name='Andy', age=30), Row(name='Justin', age=19)])\n",
    "df = rdd.toDF()\n",
    "df.show()\n",
    "+-------+----+\n",
    "|   name| age|\n",
    "+-------+----+\n",
    "|Michael|null|\n",
    "|   Andy|  30|\n",
    "| Justin|  19|\n",
    "+-------+----+\n",
    "```\n",
    "\n",
    "### Specifying the schema\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "rdd = sc.textFile(\"examples/src/main/resources/people.txt\") \n",
    "rdd.collect()\n",
    "['Michael, 29', 'Andy, 30', 'Justin, 19']\n",
    "\n",
    "rdd = rdd.map(lambda x: x.split(',')).map(lambda x: (x[0], int(x[1])))\n",
    "rdd.collect()\n",
    "[('Michael', 29), ('Andy', 30), ('Justin', 19)]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", LongType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.printSchema()\n",
    "root\n",
    " |-- name: string (nullable = false)\n",
    " |-- age: long (nullable = true)\n",
    "    \n",
    "df.createOrReplaceTempView(\"people\")\n",
    "spark.sql(\"SELECT * FROM people WHERE age > 20\").show()\n",
    "+-------+---+\n",
    "|   name|age|\n",
    "+-------+---+\n",
    "|Michael| 29|\n",
    "|   Andy| 30|\n",
    "+-------+---+\n",
    "```\n",
    "\n",
    "\n",
    "Another example:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType\n",
    "\n",
    "rdd = sc.parallelize(['\"x\",\"y\",\"z\"', '2,3,1', '3,5,2', '8,3,4'])\n",
    "header = rdd.first()\n",
    "rdd = rdd.filter(lambda r: r != header).map(lambda r: r.split(',')).map(lambda r: [int(x) for x in r])\n",
    "schema = StructType([StructField(c[1:-1], IntegerType(), True) for c in header.split(',')])\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.show()\n",
    "+---+---+---+\n",
    "|  x|  y|  z|\n",
    "+---+---+---+\n",
    "|  2|  3|  1|\n",
    "|  3|  5|  2|\n",
    "|  8|  3|  4|\n",
    "+---+---+---+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "\n",
    "## dropDuplicates()\n",
    "\n",
    "dropDuplicates(subset=None)\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame([\n",
    "   (1, 10.2, 'a'),\n",
    "   (2, 15.8, 'b'),\n",
    "   (3, 4.5, None),\n",
    "   (2, 15.8, 'b'),\n",
    "   (3, 10.2, 'a'),\n",
    "   (1, 18.3, 'b')], ['id', 'score', 'category'])\n",
    "\n",
    "# Distinct rows:\n",
    "if df.count() != df.distinct().count(): \n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "# Distinct (score, category):\n",
    "subset = [c for c in df.columns if c != 'id']\n",
    "\n",
    "if df.count() != df.select(subset).distinct().count():\n",
    "    df = df.dropDuplicates(subset)\n",
    "\n",
    "df.show()\n",
    "+---+-----+--------+\n",
    "| id|score|category|\n",
    "+---+-----+--------+\n",
    "|  1| 18.3|       b|\n",
    "|  3|  4.5|    null|\n",
    "|  1| 10.2|       a|\n",
    "|  2| 15.8|       b|\n",
    "+---+-----+--------+ \n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "df = df.withColumn('unique_id', F.monotonically_increasing_id())\n",
    "df.show()\n",
    "+---+-----+--------+-------------+\n",
    "| id|score|category|    unique_id|\n",
    "+---+-----+--------+-------------+\n",
    "|  1| 18.3|       b| 231928233984|\n",
    "|  3|  4.5|    null| 231928233985|\n",
    "|  1| 10.2|       a|1099511627776|\n",
    "|  2| 15.8|       b|1348619730944|\n",
    "+---+-----+--------+-------------+\n",
    "```\n",
    "\n",
    "## na.drop(), na.fill()\n",
    "\n",
    "* dropna() or na.drop(): how='any', thresh=None, subset=None\n",
    "\n",
    "* fillna() or na.fill(): value, subset=None\n",
    "\n",
    "fillna(value, subset=None), alias for na.fill()\n",
    "    \n",
    "```mysql\n",
    "df.na.fill({'age': 50, 'name': 'unknown'})\n",
    "```\n",
    "\n",
    "* Find the number of missing values in each row:\n",
    "\n",
    "```mysql\n",
    "df.rdd.map(lambda r: sum([c == None for c in r])).collect()\n",
    "```\n",
    "\n",
    "## outliers\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df = spark.range(1,11) \\\n",
    "  .withColumn('x', F.round(10*F.rand(),2)) \\\n",
    "  .withColumn('y', F.round(10*F.rand(),2))\n",
    "\n",
    "df.show()\n",
    "+---+----+----+\n",
    "| id|   x|   y|\n",
    "+---+----+----+\n",
    "|  1|3.81|9.86|\n",
    "|  2|0.56|7.07|\n",
    "| ...         |\n",
    "|  9|9.46|5.27|\n",
    "| 10| 4.5| 2.0|\n",
    "+---+----+----+\n",
    "\n",
    "bounds = dict()\n",
    "for c in ['x','y']:\n",
    "    bounds[c] = df.approxQuantile(c, [0.2,0.8], 0.05)\n",
    "bounds\n",
    "{'x': [0.56, 8.15], 'y': [2.0, 8.6]}\n",
    "\n",
    "is_outlier = df.select('id', *[ ( (df[c] < bounds[c][0]) | (df[c] > bounds[c][1]) ).alias(c + '_out') for c in ['x','y'] ])\n",
    "is_outlier.show()\n",
    "+---+-----+-----+\n",
    "| id|x_out|y_out|\n",
    "+---+-----+-----+\n",
    "|  1|false| true|\n",
    "|  2|false|false|\n",
    "| ...           |\n",
    "|  9| true|false|\n",
    "| 10|false|false|\n",
    "+---+-----+-----+\n",
    "\n",
    "\n",
    "df.join(is_outlier, on='id').filter(\"x_out = 'false'\").select('id','x').show()\n",
    "df.join(is_outlier, on='id').filter(\"y_out = 'false'\").select('id','y').show()\n",
    "df.join(is_outlier, on='id').filter(\"x_out = 'false' and y_out = 'false'\").select('id','x','y').show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark functions\n",
    "\n",
    "## spark.range()\n",
    "\n",
    "```python\n",
    "spark.range(1,6,2).show()\n",
    "+---+\n",
    "| id|\n",
    "+---+\n",
    "|  1|\n",
    "|  3|\n",
    "|  5|\n",
    "+---+\n",
    "```\n",
    "\n",
    "## pyspark.sql.functions\n",
    "\n",
    "pyspark.sql.functions is a collection of built-in functions.\n",
    "\n",
    "```python\n",
    "import pyspark.sql.functions as F\n",
    "```\n",
    "\n",
    "### F.when().otherwise() \n",
    "\n",
    "```python\n",
    "df.withColumn('score', F.when(F.col('score') > 80, F.col('score')).otherwise(0))\n",
    "```\n",
    "\n",
    "### F.udf()\n",
    "\n",
    "udf(f=None, returnType=StringType) creates a user defined function (UDF).\n",
    "\n",
    "The user-defined functions are considered deterministic by default. If your function is not deterministic, call `asNondeterministic` on the user defined function.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import IntegerType\n",
    "import random\n",
    "random_udf = F.udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
    "\n",
    "type(random_udf())\n",
    "<class 'pyspark.sql.column.Column'>\n",
    "\n",
    "random_udf.func()   # 52\n",
    "random_udf.func()   # 7\n",
    "\n",
    "\n",
    "slen = F.udf(lambda s: len(s), IntegerType())  # slen.func(\"John\") returns 4.\n",
    "\n",
    "@F.udf               # By default, returnType is StringType().\n",
    "def to_upper(s): \n",
    "    if s is not None: return s.upper()         # to_upper.func(\"John\") returns 'JOHN'.\n",
    "\n",
    "@F.udf(returnType=IntegerType())               \n",
    "def add_one(x):                                # add_one.func(9) returns 10.\n",
    "    if x is not None: return x + 1\n",
    "\n",
    "welcome = F.udf(lambda a, b: a +', ' + b)\n",
    "\n",
    "df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
    "\n",
    "df.select(slen(\"name\").alias(\"name_len\"), \n",
    "          to_upper(\"name\"), \n",
    "          add_one(\"age\").alias(\"age+1\"), \n",
    "          welcome(\"name\", F.lit(\"Good morning!\")).alias(\"message\")).show()\n",
    "+--------+--------------+-----+--------------------+\n",
    "|name_len|to_upper(name)|age+1|             message|\n",
    "+--------+--------------+-----+--------------------+\n",
    "|       8|      JOHN DOE|   22|John Doe, Good mo...|\n",
    "+--------+--------------+-----+--------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLlib\n",
    "\n",
    "\n",
    "## Basic ML procedure\n",
    "\n",
    "```python\n",
    "# Here data is an RDD object whose rows are LabeledPoint objects.\n",
    "# We consider a binary classification problem.\n",
    "\n",
    "# Split data\n",
    "data_train, data_test = data.randomSplit([0.8,0.2])\n",
    "\n",
    "# Train a model\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "model = LogisticRegressionWithLBFGS.train(data_train, iterations=10)\n",
    "\n",
    "# Prediction\n",
    "preds = model.predict(data_test.map(lambda row: row.features))\n",
    "results = data_test.map(lambda row: row.label).zip(preds).map(lambda row: (row[0], row[1]*1.0))\n",
    "\n",
    "# Evaluation\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics \n",
    "scores = BinaryClassificationMetrics(results)\n",
    "print(scores.areaUnderPR)     # precision-recall curve\n",
    "print(scores.areaUnderROC)\n",
    "```\n",
    "\n",
    "## pyspark.mllib.stat.Statistics\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.stat import Statistics\n",
    "```\n",
    "\n",
    "### colStats()\n",
    "\n",
    "colStats(rdd) computes column-wise summary statistics for the input RDD.\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "rdd = sc.parallelize([Vectors.dense([2, 0, 1, -2]),\n",
    "                      Vectors.dense([4, 5, 0,  3]),\n",
    "                      Vectors.dense([6, 7, -3,  8])])\n",
    "cStats = Statistics.colStats(rdd)\n",
    "\n",
    "# The following are all numpy vectors of length 4.\n",
    "cStats.mean()\n",
    "cStats.variance()\n",
    "cStats.count()\n",
    "cStats.numNonzeros()\n",
    "cStats.max()\n",
    "cStats.min()\n",
    "cStats.normL1()\n",
    "cStats.normL2()\n",
    "```\n",
    "\n",
    "If the input is a DataFrame:\n",
    "\n",
    "```python\n",
    "df.show()\n",
    "+---+----+----+\n",
    "| id|   x|   y|\n",
    "+---+----+----+\n",
    "|  1|4.22|5.08|\n",
    "|  2| 5.0|0.58|\n",
    "| ...         |\n",
    "+---+----+----+\n",
    "\n",
    "rdd = df.select('x','y').rdd.map(lambda row: [x for x in row])\n",
    "cStats = Statistics.colStats(rdd)\n",
    "```\n",
    "\n",
    "### corr()\n",
    "\n",
    "```python\n",
    "# Here rdd is the one used in colStats.\n",
    "corrs = Statistics.corr(rdd)        # 4-by-4 np array\n",
    "```\n",
    "\n",
    "### chiSqTest()\n",
    "\n",
    "chiSqTest(observed, expected=None)\n",
    "\n",
    "`observed` cannot contain negative values.\n",
    "\n",
    "If `observed` is a vector containing the observed categorical counts/relative frequencies, conduct Pearson's chi-squared goodness of fit test of the observed data against the expected distribution, or againt the uniform distribution (by default), with each category having an expected frequency of `1 / len(observed)`.\n",
    "\n",
    "If `observed` is matrix, conduct Pearson's independence test on the input contingency matrix, which cannot contain negative entries or columns or rows that sum up to 0.\n",
    "\n",
    "`expected` is a vector containing the expected categorical counts/relative frequencies. `expected` is rescaled if the expected  sum differs from the `observed` sum.\n",
    "\n",
    "If `observed` is an RDD of LabeledPoint, conduct Pearson's independence test for every feature against the label across the input RDD. For each feature, the (feature, label) pairs are converted into a contingency matrix for which the chi-squared statistic is computed. All label and feature values must be categorical.\n",
    "    \n",
    "```python\n",
    "from pyspark.mllib.linalg import Vectors, Matrices\n",
    "\n",
    "observed = Vectors.dense([4, 6, 5])\n",
    "chi = Statistics.chiSqTest(observed)\n",
    "# Try: chi.statistic, chi.pValue, chi.degreesOfFreedom, chi.method, chiu.nullHypothesis\n",
    "\n",
    "observed = Vectors.dense([21, 38, 43, 80])\n",
    "expected = Vectors.dense([3, 5, 7, 20])\n",
    "chi = Statistics.chiSqTest(observed, expected)\n",
    "    \n",
    "data = [LabeledPoint(0.0, Vectors.dense([0.5, 10.0])),\n",
    "        LabeledPoint(0.0, Vectors.dense([1.5, 20.0])),\n",
    "        LabeledPoint(1.0, Vectors.dense([1.5, 30.0])),\n",
    "        LabeledPoint(0.0, Vectors.dense([3.5, 30.0])),\n",
    "        LabeledPoint(0.0, Vectors.dense([3.5, 40.0])),\n",
    "        LabeledPoint(1.0, Vectors.dense([3.5, 40.0])),]\n",
    "rdd = sc.parallelize(data, 4)\n",
    "chi = Statistics.chiSqTest(rdd)   # a list of length 2, since there are two labels.\n",
    "[chi[i].pValue for i in range(len(chi))]\n",
    "[0.6872892787909721, 0.6822703303362126]\n",
    "```\n",
    "\n",
    "\n",
    "##  pyspark.mllib.linalg\n",
    "\n",
    "### Vectors, Matrices\n",
    "\n",
    "```python\n",
    "v = Vectors.dense([1,2,3])\n",
    "v.norm(2)\n",
    "v.dot(Vectors.dense([3,0,-2]))\n",
    "\n",
    "M = Matrices.dense(2, 3, range(6))\n",
    "M.toArray()\n",
    "array([[0., 2., 4.],\n",
    "       [1., 3., 5.]])\n",
    "```\n",
    "\n",
    "## pyspark.mllib.feature\n",
    "\n",
    "### HashingTF()\n",
    "\n",
    "HashingTF(numFeatures=1048576) maps a sequence of terms to their term frequencies using the hashing trick.\n",
    "\n",
    "```python\n",
    "htf = HashingTF()\n",
    "doc = ['a',]*100+['b',]*10+['c',]\n",
    "htf.transform(doc)\n",
    "SparseVector(1048576, {238153: 100.0, 469732: 10.0, 702740: 1.0})\n",
    "```\n",
    "\n",
    "### ChiSqSelector()\n",
    "\n",
    "We can select the most predictable features.\n",
    "\n",
    "```python\n",
    "# rdd_train and rdd_test are RDD objects consisting of LabeledPoint objects.\n",
    "model = ChiSqSelector(5).fit(rdd_train)            # top five features\n",
    "labels = rdd_train.map(lambda row: row.label)\n",
    "features_selected = model.transform(rdd_train).map(lambda row: row.features)\n",
    "rdd_train_selected = labels.zip(features_selected).map(lambda row: LabelPoint(row[0], row[1]))\n",
    "\n",
    "labels = rdd_test.map(lambda row: row.label)\n",
    "features_selected = model.transform(rdd_test).map(lambda row: row.features)\n",
    "rdd_test_selected = labels.zip(features_selected).map(lambda row: LabelPoint(row[0], row[1]))\n",
    "```\n",
    "\n",
    "## pyspark.mllib.regression\n",
    "\n",
    "### LabeledPoint()\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "lp = LabeledPoint(0.0, Vectors.dense([1.5, 20.0]))\n",
    "lp.label     # 0.0\n",
    "lp.features  # DenseVector([1.5, 20.0])\n",
    "```\n",
    "\n",
    "## pyspark.mllib.tree\n",
    "\n",
    "### RandomForest\n",
    "\n",
    "```python\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "\n",
    "model = RandomForest.trainClassifier(data=rdd_train, numClasses=2, categoricalFeatureInfo={}, numTrees=10, featureSubsetStrategy='auto', impurity='entropy', maxDepth=4, maxBins=50, seed=4042)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
