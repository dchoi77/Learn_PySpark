{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "\n",
    "Similar to pandas dataframe and SQL table.\n",
    "\n",
    "Using RDDs in PySpark occurs a possibly large overhead between Python and the JVM.\n",
    "\n",
    "Using DataFrames, PySpark is often significantly fast.\n",
    "\n",
    "```python\n",
    "df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "\n",
    "type(df)\n",
    "<class 'pyspark.sql.dataframe.DataFrame'>\n",
    "\n",
    "df.show()\n",
    "+----+-------+\n",
    "| age|   name|\n",
    "+----+-------+\n",
    "|null|Michael|\n",
    "|  30|   Andy|\n",
    "|  19| Justin|\n",
    "+----+-------+\n",
    "\n",
    "type(df['age'])     # or use df.age\n",
    "<class 'pyspark.sql.column.Column'>\n",
    "\n",
    "type(df.select('age'))\n",
    "<class 'pyspark.sql.dataframe.DataFrame'>\n",
    "```\n",
    "\n",
    "## spark.createDataFrame()\n",
    "\n",
    "```python\n",
    "spark.createDataFrame([ (1,2) ], ['x', 'y']).show()\n",
    "+---+---+\n",
    "|  x|  y|\n",
    "+---+---+\n",
    "|  1|  2|\n",
    "+---+---+\n",
    "\n",
    "spark.createDataFrame([ (1,), (2,) ], ['x']).show()\n",
    "+---+\n",
    "|  x|\n",
    "+---+\n",
    "|  1|\n",
    "|  2|\n",
    "+---+\n",
    "\n",
    "from pyspark.sql import Row\n",
    "spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "+---+---------+--------+\n",
    "|  a|  intlist|mapfield|\n",
    "+---+---------+--------+\n",
    "|  1|[1, 2, 3]|[a -> b]|\n",
    "+---+---------+--------+\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, DoubleType\n",
    "data = np.random.rand(10).round(2)\n",
    "schema = StructType([StructField('value', DoubleType(), False)])\n",
    "spark.createDataFrame([(float(x),) for x in data], schema=schema).show(n=3)\n",
    "+-----+\n",
    "|value|\n",
    "+-----+\n",
    "| 0.33|\n",
    "| 0.07|\n",
    "| 0.54|\n",
    "+-----+\n",
    "\n",
    "\n",
    "schema = StructType().add(\"id\", \"integer\").add(\"country\", \"string\")\n",
    "df = spark.createDataFrame([(5, \"USA\"), (21, \"South Korea\")], schema=schema)\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
    "df.show()\n",
    "+--------+----------+-----------------+\n",
    "|long_col|string_col|       struct_col|\n",
    "+--------+----------+-----------------+\n",
    "|       1|  a string|[a nested string]|\n",
    "+--------+----------+-----------------+\n",
    "\n",
    "\n",
    "spark.createDataFrame(pd.DataFrame([[1,2],[3,4]], columns=['x','y'])).show()\n",
    "+---+---+\n",
    "|  x|  y|\n",
    "+---+---+\n",
    "|  1|  2|\n",
    "|  3|  4|\n",
    "+---+---+\n",
    "```\n",
    "\n",
    "## Methods\n",
    "\n",
    "### show(), printSchema()\n",
    "\n",
    "show(n=20, truncate=True, vertical=False)\n",
    "\n",
    "* n: number of rows to show\n",
    "* truncate=True truncates strings longer than 20 chars by default.\n",
    "* truncate=num truncates long strings to num\n",
    "\n",
    "```python\n",
    "df.show()\n",
    "+----+-------+\n",
    "| age|   name|\n",
    "+----+-------+\n",
    "|null|Michael|\n",
    "|  30|   Andy|\n",
    "|  19| Justin|\n",
    "+----+-------+\n",
    "```\n",
    "\n",
    "printSchema() prints out the schema in the tree format.\n",
    "\n",
    "```python\n",
    "df.printSchema()\n",
    "root\n",
    " |-- age: long (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    "    \n",
    "\n",
    "df = spark.read.load(\"examples/src/main/resources/users.parquet\")\n",
    "df.show()\n",
    "+------+--------------+----------------+\n",
    "|  name|favorite_color|favorite_numbers|\n",
    "+------+--------------+----------------+\n",
    "|Alyssa|          null|  [3, 9, 15, 20]|\n",
    "|   Ben|           red|              []|\n",
    "+------+--------------+----------------+\n",
    "\n",
    "df.printSchema()\n",
    "root\n",
    " |-- name: string (nullable = true)\n",
    " |-- favorite_color: string (nullable = true)\n",
    " |-- favorite_numbers: array (nullable = true)\n",
    " |    |-- element: integer (containsNull = true)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### collect(), take(), limit()\n",
    "\n",
    "collect() and take() returns a list of Row objects.\n",
    "\n",
    "limit() returns a DataFrame.\n",
    "\n",
    "```python\n",
    "df.collect()\n",
    "[Row(age=None, name='Michael'), Row(age=30, name='Andy'), Row(age=19, name='Justin')]\n",
    "\n",
    "df.take(2)\n",
    "[Row(age=None, name='Michael'), Row(age=30, name='Andy')]\n",
    "\n",
    "df.limit(2).show()\n",
    "+----+-------+\n",
    "| age|   name|\n",
    "+----+-------+\n",
    "|null|Michael|\n",
    "|  30|   Andy|\n",
    "+----+-------+\n",
    "```\n",
    "\n",
    "\n",
    "### distinct(), count(), first()\n",
    "\n",
    "distinct() returns a new DataFrame containing the distinct rows\n",
    "\n",
    "### corr(), cov(), approxQuantile(), crosstab()\n",
    "\n",
    "* df.corr() or df.stat.corr()\n",
    "* df.cov() or df.stat.cov()\n",
    "* df.approxQuantile() or df.stat.approxQuantile()\n",
    "* df.crosstab() or df.stat.crosstab()\n",
    "\n",
    "```python\n",
    "df.corr('col1', 'col2')\n",
    "```\n",
    "\n",
    "approxQuantile(col, probabilities, relativeError) calculates the approximate quantiles of numerical columns.\n",
    "\n",
    "```python\n",
    "df.approxQuantile('income', [0.25,0.5,0.75], 0.05)     # returns [Q1, median, Q3]\n",
    "```\n",
    "\n",
    "crosstab(col1, col2) computes a pair-wise frequency table of the given columns. Also known as a contingency table.\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame([('a',1),('a',2),('b',2),('b',2),('b',1),('a',1)], ['col1','col2'])\n",
    "df.crosstab('col1','col2').show()\n",
    "+---------+---+---+\n",
    "|col1_col2|  1|  2|\n",
    "+---------+---+---+\n",
    "|        b|  1|  2|\n",
    "|        a|  2|  1|\n",
    "+---------+---+---+\n",
    "```\n",
    "\n",
    "### describe()\n",
    "\n",
    "describe(*cols) computes basic statistics for numeric and string columns.\n",
    "\n",
    "```python\n",
    "df.describe(['col1', 'col2']).show()\n",
    "```\n",
    "\n",
    "### select(), selectExpr()\n",
    "\n",
    "select(*cols) projects a set of expressions and returns a new DataFrame.\n",
    "\n",
    "```python\n",
    "df.select('age').collect()\n",
    "[Row(age=None), Row(age=30), Row(age=19)]\n",
    "\n",
    "df.select((df.age+10).alias('future_age')).collect()\n",
    "[Row(future_age=None), Row(future_age=40), Row(future_age=29)]\n",
    "\n",
    "\n",
    "df.show()\n",
    "+---+---+---+---+\n",
    "| id|  x|  y|  z|\n",
    "+---+---+---+---+\n",
    "|  1|0.4|1.2|2.4|\n",
    "|  2|1.4|0.8|1.6|\n",
    "|  3|0.7|2.2|0.9|\n",
    "+---+---+---+---+\n",
    "\n",
    "df.select(*['id']+[((df[c] > 2.0) | (df[c] < 0.5)).alias(c+'_outliers') for c in ['x','y','z']]).show()\n",
    "# or use df.select('id', *[...]).show()\n",
    "+---+----------+----------+----------+\n",
    "| id|x_outliers|y_outliers|z_outliers|\n",
    "+---+----------+----------+----------+\n",
    "|  1|      true|     false|      true|\n",
    "|  2|     false|     false|     false|\n",
    "|  3|     false|      true|     false|\n",
    "+---+----------+----------+----------+\n",
    "```\n",
    "\n",
    "selectExpr() is a variant of select() that accepts SQL expressions.\n",
    "\n",
    "```python\n",
    "df.selectExpr(\"age * 2\", \"abs(age)\")\n",
    "\n",
    "df.selectExpr(\"age\", \"double(height)/double(weight) as ratio\")\n",
    "```\n",
    "\n",
    "### filter(), where()\n",
    "\n",
    "where() is an alias for filter().\n",
    "\n",
    "```python\n",
    "textFile.filter(textFile.value.contains(\"apple\"))   \n",
    "# textFile is created by pyspark.read.text(). So its column name is value.\n",
    "\n",
    "\n",
    "df.select(df.age, df.name).filter(df.age > 16).show()     # same as filter(\"age > 16\")\n",
    "+---+------+\n",
    "|age|  name|\n",
    "+---+------+\n",
    "| 30|  Andy|\n",
    "| 19|Justin|\n",
    "+---+------+\n",
    "\n",
    "df.filter(\"name like 'A%'\").show()\n",
    "+---+----+\n",
    "|age|name|\n",
    "+---+----+\n",
    "| 30|Andy|\n",
    "+---+----+\n",
    "\n",
    "\n",
    "df.filter(\"col1 = 'apple' AND col2 > 0\").groupBy(\"col3\", \"col4\").avg(\"col5\")\n",
    "```\n",
    "\n",
    "### replace(), na.replace()\n",
    "\n",
    "replace(to_replace, value, subset=None)\n",
    "\n",
    "\n",
    "### withColumn()\n",
    "\n",
    "withColumn(colName, col) returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "\n",
    "```python\n",
    "df.withColumn('age2', df.age + 2).collect()\n",
    "[Row(age=2, name='Alice', age2=4), Row(age=5, name='Bob', age2=7)]\n",
    "\n",
    "\n",
    "df.withColumn('new_col', F.lit('Good'))\n",
    "```\n",
    "\n",
    "### sampleBy()\n",
    "\n",
    "sampleBy(col, fractions, seed=None)\n",
    "\n",
    "fractions: sampling fraction for each stratum. If a stratum is not specified, we treat its fraction as zero.\n",
    "\n",
    "```python\n",
    "df.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
    "```\n",
    "\n",
    "### groupBy(), groupby(), agg(), pivot()\n",
    "\n",
    "groupby is an alias for groupBy.\n",
    "\n",
    "Using groupBy() with no input makes each row a single group. By doing so, we can use aggregate functions. The following are all same:\n",
    "\n",
    "```python\n",
    "df.groupBy().max(\"amount\")\n",
    "df.agg({\"amount\": \"max\"})\n",
    "df.agg(F.max(df.amount))\n",
    "```\n",
    "\n",
    "df.agg() is a shorthand for df.groupBy().agg().\n",
    "\n",
    "```python\n",
    "df.agg({\"age\": \"max\", \"weight\": \"skewness\"}).collect()\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.agg(F.max(df.age))\n",
    "# or\n",
    "df.agg(F.max('age'))\n",
    "\n",
    "\n",
    "df.show()\n",
    "+-----+------+\n",
    "|first|second|\n",
    "+-----+------+\n",
    "|  2.0|   4.2|\n",
    "|  3.5|   2.8|\n",
    "+-----+------+\n",
    "\n",
    "df.agg(*[( (F.max(c) - F.min(c))/F.stddev(c) ).alias(c+'_transformed') for c in df.columns]).show()\n",
    "+------------------+------------------+\n",
    "| first_transformed|second_transformed|\n",
    "+------------------+------------------+\n",
    "|1.4142135623730951| 1.414213562373095|\n",
    "+------------------+------------------+\n",
    "```\n",
    "\n",
    "pivot(pivot_col, values=None) pivots a column of the current DataFrame and perform the specified aggregation.\n",
    "\n",
    "```python\n",
    "# Compute the sum of earnings for each year by course with each course as a separate column\n",
    "\n",
    "df.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").collect()\n",
    "[Row(year=2012, dotNET=15000, Java=20000), Row(year=2013, dotNET=48000, Java=30000)]\n",
    "```\n",
    "\n",
    "In the above, we may not specify the values of \"course\" in pivot(), but it is less efficient, because Spark needs to first compute the list of distinct values internally.\n",
    "\n",
    "\n",
    "\n",
    "### join()\n",
    "\n",
    "```python\n",
    "df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height)\n",
    "df.join(df2, 'name').select(df.name, df2.height)\n",
    "\n",
    "cond = [df.name == df3.name, df.age == df3.age]\n",
    "df.join(df3, cond, 'outer').select(df.name, df3.age)\n",
    "df.join(df4, ['name', 'age']).select(df.name, df.age)\n",
    "```\n",
    "\n",
    "### sort(), orderBy()\n",
    "\n",
    "```python\n",
    "df.sort(df.age.desc())\n",
    "df.sort(\"age\", ascending=False)\n",
    "df.sort(desc(\"age\"))\n",
    "\n",
    "df.orderBy(df.age.desc())\n",
    "df.orderBy(desc(\"age\"), \"name\")\n",
    "df.orderBy([\"age\", \"name\"], ascending=[0, 1])\n",
    "```    \n",
    "\n",
    "### drop(), dropDuplicates(), drop_duplicates()\n",
    "\n",
    "```python\n",
    "df.drop('age')\n",
    "df.join(df2, df.name == df2.name, 'inner').drop(df.name)\n",
    "\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "   (1, 10.2, 'a'),\n",
    "   (2, 15.8, 'b'),\n",
    "   (3, 4.5, None),\n",
    "   (2, 15.8, 'b'),\n",
    "   (3, 10.2, 'a'),\n",
    "   (1, 18.3, 'b')], ['id', 'score', 'category'])\n",
    "\n",
    "# Distinct rows:\n",
    "if df.count() != df.distinct().count(): df = df.dropDuplicates()\n",
    "\n",
    "# Distinct (score, category):\n",
    "subset = [c for c in df.columns if c != 'id']\n",
    "if df.count() != df.select(subset).distinct().count(): df = df.dropDuplicates(subset)\n",
    "```\n",
    "\n",
    "### na.drop(), dropna(), na.fill(), fillna()\n",
    "\n",
    "```mysql\n",
    "df.na.fill({'age': 50, 'name': 'unknown'})\n",
    "```\n",
    "\n",
    "* Find the number of missing values in each row:\n",
    "\n",
    "```mysql\n",
    "df.rdd.map(lambda r: sum([c == None for c in r])).collect()\n",
    "```\n",
    "\n",
    "\n",
    "### toPandas()\n",
    "\n",
    "df.toPandas() is a pandas dataframe.\n",
    "\n",
    "\n",
    "### write.\n",
    "\n",
    "* df.write.csv()\n",
    "* df.write.json()\n",
    "* df.write.parquet()\n",
    "* df.write.text()\n",
    "* df.write.save()\n",
    "* df.write.format()\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame(...).write.csv(\"path/to/folder\", mode='append')  # the first input is not the file name\n",
    "```\n",
    "\n",
    "\n",
    "## Column operations\n",
    "\n",
    "```python\n",
    "df = spark.read.json(\"examples/src/main/resources/people.json\")\n",
    "df.columns\n",
    "['age', 'name']\n",
    "\n",
    "type(df.age)\n",
    "<class 'pyspark.sql.column.Column'>\n",
    "```\n",
    "\n",
    "### alias()\n",
    "\n",
    "```python\n",
    "df.select(df.age.alias('Age')).columns\n",
    "['Age']\n",
    "```\n",
    "\n",
    "### cast(), astype()\n",
    "\n",
    "```python\n",
    "df.dtypes[0]\n",
    "('age', 'bigint')\n",
    "\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "df = df.withColumn('age', df.age.cast(IntegerType()))\n",
    "df.dtypes[0]\n",
    "('age', 'int')\n",
    "```\n",
    "\n",
    "### isNull(), isNotNull()\n",
    "\n",
    "```python\n",
    "df.filter(df.height.isNull())\n",
    "```\n",
    "\n",
    "### isin(), between()\n",
    "\n",
    "```python\n",
    "df[df.name.isin(\"Mike\",\"Michael\")].show()\n",
    "\n",
    "df.select(df.name, df.age.between(2, 4)).show()\n",
    "```\n",
    "\n",
    "### startswith(), endswith()\n",
    "\n",
    "```python\n",
    "df.filter(df.name.startswith('Al')).collect()\n",
    "```\n",
    "\n",
    "### substr()\n",
    "\n",
    "substr(startPos, length)\n",
    "\n",
    "```python\n",
    " df.select(df.name.substr(1, 3).alias(\"col\")).collect()\n",
    "[Row(col='Mic'), Row(col='And'), Row(col='Jus')]\n",
    "```\n",
    "\n",
    "### contains()\n",
    "\n",
    "```python\n",
    "df.filter(df['value'].contains('apple')).count()\n",
    "```\n",
    "\n",
    "### when(), otherwise()\n",
    "\n",
    "```python\n",
    "df.show()\n",
    "+----+-------+\n",
    "| age|   name|\n",
    "+----+-------+\n",
    "|null|Michael|\n",
    "|  30|   Andy|\n",
    "|  19| Justin|\n",
    "+----+-------+\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "df.select(df.name, F.when(df.age > 25, 2).when(df.age > 15, 1).otherwise(0).alias('age_gp')).show()\n",
    "+-------+------+\n",
    "|   name|age_gp|\n",
    "+-------+------+\n",
    "|Michael|     0|\n",
    "|   Andy|     2|\n",
    "| Justin|     1|\n",
    "+-------+------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View & SQL query\n",
    "\n",
    "### Global temporary view\n",
    "\n",
    "A global temporary view can be shared among all sesessions.\n",
    "\n",
    "```python\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "# Use global_temp.name:\n",
    "spark.sql(\"SELECT name, age FROM global_temp.people WHERE age IS NOT NULL\").show()\n",
    "+------+---+\n",
    "|  name|age|\n",
    "+------+---+\n",
    "|  Andy| 30|\n",
    "|Justin| 19|\n",
    "+------+---+\n",
    "\n",
    "spark.newSession().sql(\"SELECT name, age FROM global_temp.people WHERE age IS NOT NULL\").show()\n",
    "# the same result as above\n",
    "```\n",
    "\n",
    "### Temporary view\n",
    "\n",
    "```python\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "spark.sql(\"SELECT name, age FROM people WHERE age IS NOT NULL\").show()\n",
    "# the same result as above\n",
    "\n",
    "spark.newSession().sql(\"SELECT name, age FROM people WHERE age IS NOT NULL\").show()\n",
    "# Error\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD to DataFrame\n",
    "\n",
    "### Inferring the schema using reflection\n",
    "\n",
    "```python\n",
    "rdd = sc.textFile(\"examples/src/main/resources/people.json\")    # RDD\n",
    "df = spark.read.json(rdd)                                       # DataFrame\n",
    "\n",
    "df.printSchema()\n",
    "root\n",
    " |-- age: long (nullable = true)\n",
    " |-- name: string (nullable = true)\n",
    "    \n",
    "    \n",
    "from pyspark.sql import Row\n",
    "rdd = sc.parallelize([Row(name='Michael',age=None), Row(name='Andy', age=30), Row(name='Justin', age=19)])\n",
    "df = rdd.toDF()\n",
    "df.show()\n",
    "+-------+----+\n",
    "|   name| age|\n",
    "+-------+----+\n",
    "|Michael|null|\n",
    "|   Andy|  30|\n",
    "| Justin|  19|\n",
    "+-------+----+\n",
    "```\n",
    "\n",
    "### Specifying the schema\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "rdd = sc.textFile(\"examples/src/main/resources/people.txt\") \n",
    "rdd.collect()\n",
    "['Michael, 29', 'Andy, 30', 'Justin, 19']\n",
    "\n",
    "rdd = rdd.map(lambda x: x.split(',')).map(lambda x: (x[0], int(x[1])))\n",
    "rdd.collect()\n",
    "[('Michael', 29), ('Andy', 30), ('Justin', 19)]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", LongType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.printSchema()\n",
    "root\n",
    " |-- name: string (nullable = false)\n",
    " |-- age: long (nullable = true)\n",
    "    \n",
    "df.createOrReplaceTempView(\"people\")\n",
    "spark.sql(\"SELECT * FROM people WHERE age > 20\").show()\n",
    "+-------+---+\n",
    "|   name|age|\n",
    "+-------+---+\n",
    "|Michael| 29|\n",
    "|   Andy| 30|\n",
    "+-------+---+\n",
    "```\n",
    "\n",
    "\n",
    "Another example:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType\n",
    "\n",
    "rdd = sc.parallelize(['\"x\",\"y\",\"z\"', '2,3,1', '3,5,2', '8,3,4'])\n",
    "header = rdd.first()\n",
    "rdd = rdd.filter(lambda r: r != header).map(lambda r: r.split(',')).map(lambda r: [int(x) for x in r])\n",
    "schema = StructType([StructField(c[1:-1], IntegerType(), True) for c in header.split(',')])\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "df.show()\n",
    "+---+---+---+\n",
    "|  x|  y|  z|\n",
    "+---+---+---+\n",
    "|  2|  3|  1|\n",
    "|  3|  5|  2|\n",
    "|  8|  3|  4|\n",
    "+---+---+---+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark functions\n",
    "\n",
    "## spark.range()\n",
    "\n",
    "```python\n",
    "spark.range(1,6,2).show()\n",
    "+---+\n",
    "| id|\n",
    "+---+\n",
    "|  1|\n",
    "|  3|\n",
    "|  5|\n",
    "+---+\n",
    "```\n",
    "\n",
    "## pyspark.sql.functions\n",
    "\n",
    "pyspark.sql.functions is a collection of built-in functions.\n",
    "\n",
    "```python\n",
    "import pyspark.sql.functions as F\n",
    "```\n",
    "\n",
    "### F.col()\n",
    "\n",
    "F.col(name) returns a Column based on the given column name.\n",
    "\n",
    "```python\n",
    "df.select(F.col('age')*10)\n",
    "# same as\n",
    "df.select(df['age']*10)\n",
    "```\n",
    "\n",
    "### F.split()\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n",
    "df.select(F.split(df.s, '[ABC]', 2).alias('s')).show()\n",
    "+-----------------+\n",
    "|                s|\n",
    "+-----------------+\n",
    "|[one, twoBthreeC]|\n",
    "+-----------------+\n",
    "df.select(F.split(df.s, '[ABC]', -1).alias('s')).show()\n",
    "+-------------------+\n",
    "|                  s|\n",
    "+-------------------+\n",
    "|[one, two, three, ]|\n",
    "+-------------------+\n",
    "```\n",
    "\n",
    "### F.explode()\n",
    "\n",
    "```python\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "\n",
    "df.select(F.explode(df.intlist).alias(\"anInt\")).show()\n",
    "+-----+\n",
    "|anInt|\n",
    "+-----+\n",
    "|    1|\n",
    "|    2|\n",
    "|    3|\n",
    "+-----+\n",
    "\n",
    "df.select(F.explode(df.mapfield).alias(\"key\", \"value\")).show()\n",
    "+---+-----+\n",
    "|key|value|\n",
    "+---+-----+\n",
    "|  a|    b|\n",
    "+---+-----+\n",
    "\n",
    "\n",
    "textFile.select(F.explode(F.split(textFile.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
    "```\n",
    "\n",
    "### F.rand(), F.randn()\n",
    "\n",
    "rand(seed) generates a random column with i.i.d. samples uniformly distributed in [0.0, 1.0).\n",
    "\n",
    "randn(seed) generates a column with i.i.d. samples from the standard normal distribution.\n",
    "\n",
    "```python\n",
    "df.withColumn('rand', F.rand(seed=42) * 3)\n",
    "df.withColumn('randn', F.randn(seed=42) * 3)\n",
    "```\n",
    "\n",
    "\n",
    "### F.when().otherwise() \n",
    "\n",
    "```python\n",
    "df.withColumn('score', F.when(F.col('score') > 80, F.col('score')).otherwise(0))\n",
    "```\n",
    "\n",
    "### F.monotonically_increasing_id()\n",
    "\n",
    "```python\n",
    "df.withColumn('unique_id', F.monotonically_increasing_id()).show()\n",
    "+---+-----+--------+-------------+\n",
    "| id|score|category|    unique_id|\n",
    "+---+-----+--------+-------------+\n",
    "|  1| 18.3|       b| 231928233984|\n",
    "|  3|  4.5|    null| 231928233985|\n",
    "|  1| 10.2|       a|1099511627776|\n",
    "|  2| 15.8|       b|1348619730944|\n",
    "+---+-----+--------+-------------+\n",
    "```\n",
    "\n",
    "### F.udf()\n",
    "\n",
    "udf(f=None, returnType=StringType) creates a user defined function (UDF).\n",
    "\n",
    "We create a standard Python function, wrap it with F.udf(), and use it on a DataFrame.\n",
    "\n",
    "We call a udf with a column name of a DataFrame, and the input to the udf is a value of each row in the column.\n",
    "\n",
    "```python\n",
    "slen = F.udf(lambda s: len(s), IntegerType())  # slen.func(\"John\") returns 4.\n",
    "\n",
    "@F.udf               # By default, returnType is StringType().\n",
    "def to_upper(s): \n",
    "    if s is not None: return s.upper()         # to_upper.func(\"John\") returns 'JOHN'.\n",
    "\n",
    "@F.udf(returnType=IntegerType())               \n",
    "def add_one(x):                                # add_one.func(9) returns 10.\n",
    "    if x is not None: return x + 1\n",
    "\n",
    "welcome = F.udf(lambda a, b: a +', ' + b)\n",
    "\n",
    "df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
    "\n",
    "df.select(slen(\"name\").alias(\"name_len\"), \n",
    "          to_upper(\"name\"), \n",
    "          add_one(\"age\").alias(\"age+1\"), \n",
    "          welcome(\"name\", F.lit(\"Good morning!\")).alias(\"message\")).show()\n",
    "+--------+--------------+-----+--------------------+\n",
    "|name_len|to_upper(name)|age+1|             message|\n",
    "+--------+--------------+-----+--------------------+\n",
    "|       8|      JOHN DOE|   22|John Doe, Good mo...|\n",
    "+--------+--------------+-----+--------------------+\n",
    "\n",
    "\n",
    "\n",
    "@F.udf(returnType=StringType())\n",
    "def grade(score):\n",
    "    if score < 80:\n",
    "        return 'C'\n",
    "    elif score < 90:\n",
    "        return 'B'\n",
    "    elif score <= 100:\n",
    "        return 'A'\n",
    "    else:\n",
    "        return 'D'\n",
    "df.select(grade(df['score'].cast(IntegerType())))     # if df['score'] is not integer\n",
    "```\n",
    "\n",
    "The user-defined functions are considered deterministic by default. If your function is not deterministic, call `asNondeterministic` on the user defined function.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import IntegerType\n",
    "import random\n",
    "random_udf = F.udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
    "\n",
    "type(random_udf())\n",
    "<class 'pyspark.sql.column.Column'>\n",
    "\n",
    "random_udf.func()   # 52\n",
    "random_udf.func()   # 7\n",
    "```\n",
    "\n",
    "### F.pandas_udf()\n",
    "\n",
    "A Python UDF is executed row by row, but pandas UDFs allow vectorized operations that can increase performance.\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame(\n",
    "    [[1, \"a string\", (\"a nested string\",)]],\n",
    "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
    "df.show()\n",
    "+--------+----------+-----------------+\n",
    "|long_col|string_col|       struct_col|\n",
    "+--------+----------+-----------------+\n",
    "|       1|  a string|[a nested string]|\n",
    "+--------+----------+-----------------+\n",
    "\n",
    "\n",
    "@F.pandas_udf(\"col1 string, col2 long\")\n",
    "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
    "    s3['col2'] = s1 + s2.str.len()\n",
    "    return s3\n",
    "\n",
    "df.select(func(\"long_col\", \"string_col\", \"struct_col\").alias('result')).show()\n",
    "+--------------------+\n",
    "|              result|\n",
    "+--------------------+\n",
    "|[a nested string, 9]|\n",
    "+--------------------+\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame([1,2,3], columns=[\"x\"]))\n",
    "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
    "    return a * b\n",
    "multiply = F.pandas_udf(multiply_func, returnType=LongType())\n",
    "df.select(multiply(F.col(\"x\"), F.col(\"x\")).alias('result')).show()\n",
    "+------+\n",
    "|result|\n",
    "+------+\n",
    "|     1|\n",
    "|     4|\n",
    "|     9|\n",
    "+------+\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pyspark.sql.Window\n",
    "\n",
    "```python\n",
    "from pyspark.sql import Window\n",
    "\n",
    "tup = [(1, \"a\"), (1, \"a\"), (2, \"a\"), (2, \"a\"), (1, \"b\"), (3, \"b\"), (4,\"b\")]\n",
    "df = spark.createDataFrame(tup, [\"id\", \"category\"])\n",
    "window = Window.partitionBy(\"category\").orderBy(\"id\").rangeBetween(Window.currentRow, 1)\n",
    "df.withColumn(\"sum\", F.sum(\"id\").over(window)).sort('category','id').show()\n",
    "+---+--------+---+\n",
    "| id|category|sum|\n",
    "+---+--------+---+\n",
    "|  1|       a|  6|    # 1+1+2+2\n",
    "|  1|       a|  6|    # 1+1+2+2\n",
    "|  2|       a|  4|    # 4+4\n",
    "|  2|       a|  4|    # 4+4\n",
    "|  1|       b|  1|    # 1\n",
    "|  3|       b|  7|    # 3+4\n",
    "|  4|       b|  4|    # 4\n",
    "+---+--------+---+\n",
    "```\n",
    "\n",
    "Note that `orderBy(\"id\").rangeBetween(Window.currentRow, 1)` does not mean that the range is between the current row and the next row. It means that the window is between the first row whose is same as the current row and the last row whose id is (the current id + 1). \n",
    "\n",
    "Use `Window.unboundedPreceding`, `Window.unboundedFollowing`, and `Window.currentRow` to specify special boundary values, rather than using integral values directly.\n",
    "\n",
    "```python\n",
    "window = Window.partitionBy(\"category\").orderBy(\"id\").rangeBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "df.withColumn(\"sum\", F.sum(\"id\").over(window)).sort('category','id').show()\n",
    "+---+--------+---+\n",
    "| id|category|sum|\n",
    "+---+--------+---+\n",
    "|  1|       a|  2|\n",
    "|  1|       a|  2|\n",
    "|  2|       a|  6|\n",
    "|  2|       a|  6|\n",
    "|  1|       b|  1|\n",
    "|  3|       b|  4|\n",
    "|  4|       b|  8|\n",
    "+---+--------+---+\n",
    "```\n",
    "\n",
    "What is the difference between the revenue of each product and the revenue of the best selling product in the same category as that product? (from databricks.com)\n",
    "\n",
    "```python\n",
    "windowSpec = Window.partitionBy(df['category'])\\\n",
    "  .orderBy(df['revenue'].desc())\\\n",
    "  .rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "revenue_difference = \n",
    "df.select('product', \n",
    "          'category', \n",
    "          'revenue', \n",
    "          (F.max('revenue').over(windowSpec) - df['revenue']).alias(\"revenue_difference\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load, Write, Structured streaming\n",
    "\n",
    "\n",
    "## spark.read.\n",
    "\n",
    "* text(paths, wholetext=False, lineSep=None, ...)\n",
    "* csv(...)\n",
    "* parquet(...)\n",
    "\n",
    "```python\n",
    "textFile = spark.read.text(\"README.md\")    # DataFrame with column name 'value'\n",
    "```\n",
    "\n",
    "## Structured streaming\n",
    "\n",
    "```python\n",
    "df.show()\n",
    "+------+----+---------+\n",
    "|  name| age|      job|\n",
    "+------+----+---------+\n",
    "| Jorge|  30|Developer|\n",
    "|   Bob|  32|Developer|\n",
    "|  John|  28| Engineer|\n",
    "|Gloria|null|   Artist|\n",
    "+------+----+---------+\n",
    "\n",
    "df.printSchema()\n",
    "root\n",
    " |-- name: string (nullable = true)\n",
    " |-- age: long (nullable = true)\n",
    " |-- job: string (nullable = true)\n",
    "    \n",
    "df.write.csv(\"people_csv\", mode=\"append\")\n",
    "\n",
    "sdf = spark.readStream.csv(path='people_csv', schema=df.schema)\n",
    "\n",
    "sdf.isStreaming        # True\n",
    "\n",
    "query = sdf.groupBy('job').count()\\\n",
    "  .writeStream.start(queryName='num_jobs', outputMode='complete', format='memory')\n",
    "\n",
    "spark.sql(\"SELECT * FROM num_jobs\").show()\n",
    "+---------+-----+\n",
    "|      job|count|\n",
    "+---------+-----+\n",
    "|Developer|    2|\n",
    "|   Artist|    1|\n",
    "| Engineer|    1|\n",
    "+---------+-----+\n",
    "\n",
    "df2 = spark.createDataFrame([(\"Pam\", 29, \"Engineer\"), (\"Tom\", 35, \"Marketer\"), (\"Ben\", 33, \"Marketer\")], schema = df.schema)\n",
    "df2.write.csv(\"people_csv\", mode=\"append\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM num_jobs\").show()\n",
    "+---------+-----+\n",
    "|      job|count|\n",
    "+---------+-----+\n",
    "|Developer|    2|\n",
    "|   Artist|    1|\n",
    "| Engineer|    2|\n",
    "| Marketer|    2|\n",
    "+---------+-----+\n",
    "```\n",
    "\n",
    "We can join a stream dataframe with a static dataframe:\n",
    "\n",
    "```python\n",
    "room_info = spark.createDataFrame([('Developer', 'A302'), ('Engineer', 'A300'), ('Artist','A303'), ('Marketer', 'B250')], ['job', 'room_no'])\n",
    "\n",
    "sdf_joined = sdf.join(room_info, 'job')\n",
    "sdf_joined.isStreaming      # True\n",
    "\n",
    "query = joined_df.writeStream.start(queryName='join_query', outputMode='append', format='memory')\n",
    "spark.sql(\"SELECT * FROM join_query\").show()\n",
    "+---------+------+----+-------+\n",
    "|      job|  name| age|room_no|\n",
    "+---------+------+----+-------+\n",
    "|Developer|   Bob|  32|   A302|\n",
    "|Developer| Jorge|  30|   A302|\n",
    "| Engineer|   Pam|  29|   A300|\n",
    "| Engineer|  John|  28|   A300|\n",
    "|   Artist|Gloria|null|   A303|\n",
    "| Marketer|   Ben|  33|   B250|\n",
    "| Marketer|   Tom|  35|   B250|\n",
    "+---------+------+----+-------+\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
